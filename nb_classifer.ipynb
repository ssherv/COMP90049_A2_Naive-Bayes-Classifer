{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Naive Bayes [20 marks]\n",
    "\n",
    "Student Name: Shervyn Singh\n",
    "\n",
    "Student ID: 1236509\n",
    "\n",
    "## General info\n",
    "\n",
    "<b>Due date</b>: Friday, 2 September 2022, 5pm\n",
    "\n",
    "<b>Submission method</b>: Canvas submission\n",
    "\n",
    "<b>Submission materials</b>: completed copy of this iPython notebook\n",
    "\n",
    "<b>Late submissions</b>: -10% per day up to 5 days (both weekdays and weekends count)\n",
    "<ul>\n",
    "    <li>one day late, -2.0;</li>\n",
    "    <li>two days late, -4.0;</li>\n",
    "    <li>three days late, -6.0;</li>\n",
    "    <li>four days late, -8.0;</li>\n",
    "    <li>five days late, -10.0;</li>\n",
    "</ul>\n",
    "\n",
    "<b>Marks</b>: 20% of mark for class. \n",
    "\n",
    "<b>Materials</b>: See [Using Jupyter Notebook and Python page](https://canvas.lms.unimelb.edu.au/courses/126693/pages/python-and-jupyter-notebooks?module_item_id=3950453) on Canvas (under Modules> Coding Resources) for information on the basic setup required for this class, including an iPython notebook viewer.If your iPython notebook doesn't run on the marker's machine, you will lose marks. <b> You should use Python 3</b>.  \n",
    "\n",
    "\n",
    "<b>Evaluation</b>: Your iPython notebook should run end-to-end without any errors in a reasonable amount of time, and you must follow all instructions provided below, including specific implementation requirements and instructions for what needs to be printed (please avoid printing output we don't ask for). You should implement functions for the skeletons listed below. You may implement any number of additional (helper) functions. You should leave the output from running your code in the iPython notebook you submit, to assist with marking. The amount each section is worth is given in parenthesis after the instructions. \n",
    "\n",
    "You will be marked not only on the correctness of your methods, but also the quality and efficiency of your code: in particular, you should be careful to use Python built-in functions and operators when appropriate and pick descriptive variable names that adhere to <a href=\"https://www.python.org/dev/peps/pep-0008/\">Python style requirements</a>. If you think it might be unclear what you are doing, you should comment your code to help the marker make sense of it. We reserve the right to deduct up to 4 marks for unreadable or excessively inefficient code.\n",
    "\n",
    "7 of the marks available for this Project will be assigned to whether the five specified Python functions work in a manner consistent with the materials from COMP90049. Any other implementation will not be directly assessed (except insofar as it is required to make these five functions work correctly).\n",
    "\n",
    "13 of the marks will be assigned to your responses to the questions, in terms of both accuracy and insightfulness. We will be looking for evidence that you have an implementation that allows you to explore the problem, but also that you have thought deeply about the data and the behaviour of the Naive Bayes classifier.\n",
    "\n",
    "<b>Updates</b>: Any major changes to the assignment will be announced via Canvas. Minor changes and clarifications will be announced on the discussion board (Piazza -> Assignments -> A2); we recommend you check it regularly.\n",
    "\n",
    "<b>Academic misconduct</b>: While you may discuss this homework in general terms with other students, it ultimately is still an individual task. Reuse of code or other instances of clear influence will be considered cheating. Please check the <a href=\"https://canvas.lms.unimelb.edu.au/courses/126693/modules#module_734188\">CIS Academic Honesty training</a> for more information. We will be checking submissions for originality and will invoke the University’s <a href=\"http://academichonesty.unimelb.edu.au/policy.html\">Academic Misconduct policy</a> where inappropriate levels of collusion or plagiarism are deemed to have taken place.\n",
    "\n",
    "**IMPORTANT**\n",
    "\n",
    "Please carefully read and fill out the <b>Authorship Declaration</b> form at the bottom of the page. Failure to fill out this form results in the following deductions: \n",
    "<UL TYPE=”square”>\n",
    "<LI>missing Authorship Declaration at the bottom of the page, -10.0\n",
    "<LI>incomplete or unsigned Authorship Declaration at the bottom of the page, -5.0\n",
    "</UL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Base code [7 marks]\n",
    "\n",
    "Instructions\n",
    "1. Do **not** shuffle the data set\n",
    "2. Treat the features as nominal and use them as provided (e.g., do **not** convert them to other feature types, such as numeric ones). Implement a Naive Bayes classifier with appropriate likelihood function for the data.\n",
    "3. You should implement the Naive Bayes classifier from scratch. Do **not** use existing implementations/learning algorithms. You must use epsilon smoothing strategy as discussed in the Naive Bayes lecture. \n",
    "4. Apart from the instructions in point 3, you may use libraries to help you with data reading, representation, maths or evaluation.\n",
    "5. Ensure that all and only required information is printed, as indicated in the final three code cells. Failure to adhere to print the required information will result in **[-1 mark]** per case. *(We don't mind details like you print a list or several numbers -- just make sure the information is displayed so that it's easily accessible)*\n",
    "6. Please place the jupyter notebook into the same folder as the input data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all libraries\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should open a csv file and read the data into a useable format [0.5 mark]\n",
    "def preprocess(filename):\n",
    "    dataframe = pd.read_csv(filename)\n",
    "    dataframe = dataframe.iloc[: , 1:] # Remove the instance identifier\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This function should build a supervised NB model [3 marks]\n",
    "# NOTE: This implementation draws heavily from - http://deebuls.github.io/Naive-Bayes-Pandas.html\n",
    "\n",
    "def train(data):\n",
    "    \n",
    "    # Calculate prior probabilities of each label\n",
    "    priors = data.groupby('label').size().div(len(data))\n",
    "    \n",
    "    # Return a list of features \n",
    "    features = list(data.columns[:-1])\n",
    "    \n",
    "    # Calculate likelihood (conditional probabilities) of a feature given a class label\n",
    "    likelihood = dict()\n",
    "    for ft in features:\n",
    "        likelihood[ft] = data.groupby(['label', ft]).size().div(len(data)).div(priors)\n",
    "    \n",
    "    return priors, likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should predict the class for a set of instances, based on a trained model [1.5 marks]\n",
    "def predict(train_model, data):\n",
    "    \n",
    "    # Extract the probabilities calulcated using train()\n",
    "    priors = train_model[0]\n",
    "    conditionals = train_model[1]\n",
    "    \n",
    "    # separate data into usable lists\n",
    "    features = list(data.columns[:-1])\n",
    "    labels = list(data[\"label\"].unique())\n",
    "    instances = data.iloc[:,:-1].values\n",
    "    \n",
    "    # set up lists to record the final posterier probabilities and associated prediction of each instance \n",
    "    posterior_probabilities = []\n",
    "    label_predictions = []\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # go through each instance one at a time\n",
    "    for instance in instances:\n",
    "\n",
    "        # set up dictionary to the probability of each class prediction\n",
    "        likelihoods = {label:0 for label in labels}\n",
    "\n",
    "        # enumerate all the probabilities of the feature set for each class label\n",
    "        for i in range(len(labels)):\n",
    "\n",
    "            for j in range(len(features)):\n",
    "                     \n",
    "                try: # cannot calculate the probability if a feature/value/label is not observed in the data-set\n",
    "                    raw_prob = conditionals[features[j]][labels[i]][instance[j]]\n",
    "                except:\n",
    "                    # epsilon smoothing -- E = 1 / N * 100\n",
    "                    raw_prob = 1 / (len(instance) * 100)\n",
    "\n",
    "                # log transform the probabilities to avoid underflow \n",
    "                log_prob = np.log(raw_prob)\n",
    "\n",
    "                # add the probability to the dictionary for storage\n",
    "                likelihoods[labels[i]] += log_prob\n",
    "\n",
    "        # add the prior probability to our record of conditional probabilities (to find the posterior probability)\n",
    "        for i in range(len(labels)):\n",
    "            likelihoods[labels[i]] += np.log(priors[labels[i]])\n",
    "\n",
    "        # append the posterior probability for this feature set to a list to be used later        \n",
    "        posterior_probabilities.append(likelihoods)       \n",
    "\n",
    "        # set the predicted class of this feature to the most frequently label observed\n",
    "        prediction = max(likelihoods, key=lambda key: likelihoods[key])\n",
    "\n",
    "        # append the prediction to a list for use later\n",
    "        label_predictions.append(prediction)\n",
    "\n",
    "    return posterior_probabilities, label_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should evaluate a set of predictions [1 mark]\n",
    "def evaluate(eval_metric, true_data, predictions, pos_label, average):\n",
    "\n",
    "    if eval_metric == \"accuracy\":\n",
    "        return sklearn.metrics.accuracy_score(true_data, predictions)\n",
    "\n",
    "    if eval_metric == \"precision\":\n",
    "        if pos_label == None: # multiclass dataset\n",
    "            return sklearn.metrics.precision_score(true_data, predictions, average=average)\n",
    "        else: # binary dataset\n",
    "            return sklearn.metrics.precision_score(true_data, predictions, pos_label=pos_label)\n",
    "    \n",
    "    if eval_metric == \"recall\":\n",
    "        if pos_label==None: # multiclass dataset\n",
    "            return sklearn.metrics.recall_score(true_data, predictions, average=average)\n",
    "        else: # binary dataset\n",
    "            return sklearn.metrics.recall_score(true_data, predictions, pos_label=pos_label)\n",
    "    \n",
    "    if eval_metric == \"f-score\":\n",
    "        if pos_label==None: # multiclass dataset\n",
    "            return sklearn.metrics.f1_score(true_data, predictions, average=average)\n",
    "        else: # binary dataset\n",
    "            return sklearn.metrics.f1_score(true_data, predictions, pos_label=pos_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bank Marketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8874142888741429\n",
      "Precision (yes):  0.525\n",
      "Precision (no):  0.9077318383555244\n",
      "Recall (yes):  0.2418426103646833\n",
      "Recall (no):  0.9715\n",
      "F-score (yes):  0.3311432325886991\n",
      "F-score (no):  0.9385339934790485\n",
      "\n",
      "Feature vectors of instances [0, 1, 2]: \n",
      " [['unemployed' 'married' 'primary' 'no' 'no' 'no' 'cellular' 'oct'\n",
      "  'unknown']\n",
      " ['services' 'married' 'secondary' 'no' 'yes' 'yes' 'cellular' 'may'\n",
      "  'failure']\n",
      " ['management' 'single' 'tertiary' 'no' 'yes' 'no' 'cellular' 'apr'\n",
      "  'failure']]\n",
      "\n",
      "Number of instances (N):  4521\n",
      "Number of features (F):  9\n",
      "Number of labels (L):  2\n",
      "\n",
      "\n",
      "Predicted class probabilities for instance N-3:  {'no': -6.7116077635370655, 'yes': -8.583110831432391}\n",
      "Predicted class for instance N-3:  no\n",
      "\n",
      "Predicted class probabilities for instance N-2:  {'no': -10.619918711707394, 'yes': -11.68220339799834}\n",
      "Predicted class for instance N-2:  no\n",
      "\n",
      "Predicted class probabilities for instance N-1:  {'no': -14.88701380580801, 'yes': -16.28697922108946}\n",
      "Predicted class for instance N-1:  no\n"
     ]
    }
   ],
   "source": [
    "# This cell should act as your \"main\" function where you call the above functions \n",
    "# on the full Bank Marketing data set, and print the evaluation score. [0.33 marks]\n",
    "\n",
    "# First, read in the data and apply your NB model to the Bank Marketing data\n",
    "bank_data = preprocess('bank-marketing.csv')\n",
    "bank_model = train(bank_data)\n",
    "bank_predictions = predict(bank_model, bank_data)\n",
    "\n",
    "# Second, print the full evaluation results from the evaluate() function\n",
    "# This is the true labels of the dataset - to be used to evaluate the algorithm\n",
    "bank_truth = list(bank_data['label'])\n",
    "\n",
    "# ACCURACY SCORE\n",
    "bank_accuracy =  evaluate(\"accuracy\", bank_truth, bank_predictions[1], pos_label=None, average=None)\n",
    "print(\"Accuracy: \", bank_accuracy)\n",
    "\n",
    "# PRECISION\n",
    "bank_precision_yes =  evaluate(\"precision\", bank_truth, bank_predictions[1], pos_label=\"yes\", average=None)\n",
    "bank_precision_no = evaluate(\"precision\", bank_truth, bank_predictions[1], pos_label=\"no\", average=None)\n",
    "print(\"Precision (yes): \", bank_precision_yes)\n",
    "print(\"Precision (no): \", bank_precision_no)\n",
    "\n",
    "# RECALL\n",
    "bank_recall_yes =  evaluate(\"recall\", bank_truth, bank_predictions[1], pos_label=\"yes\", average=None)\n",
    "bank_recall_no = evaluate(\"recall\", bank_truth, bank_predictions[1], pos_label=\"no\", average=None)\n",
    "print(\"Recall (yes): \", bank_recall_yes)\n",
    "print(\"Recall (no): \", bank_recall_no)\n",
    "\n",
    "# F-SCORE\n",
    "bank_fscore_yes =  evaluate(\"f-score\", bank_truth, bank_predictions[1], pos_label=\"yes\", average=None)\n",
    "bank_fscore_no = evaluate(\"f-score\", bank_truth, bank_predictions[1], pos_label=\"no\", average=None)\n",
    "print(\"F-score (yes): \", bank_fscore_yes)\n",
    "print(\"F-score (no): \", bank_fscore_no)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "# Third, print data statistics and model predictions, as instructed below \n",
    "# N is the total number of instances, F the total number of features, L the total number of labels\n",
    "# The \"class probabilities\" may be unnormalized\n",
    "\n",
    "print(\"Feature vectors of instances [0, 1, 2]: \\n\", bank_data.iloc[:,:-1].values[:3])\n",
    "\n",
    "print(\"\\nNumber of instances (N): \", bank_data.shape[0])\n",
    "print(\"Number of features (F): \", len(list(bank_data.columns[:-1])))\n",
    "print(\"Number of labels (L): \", len(list(bank_data[\"label\"].unique())))\n",
    "\n",
    "print(\"\\n\\nPredicted class probabilities for instance N-3: \", bank_predictions[0][-3])\n",
    "print(\"Predicted class for instance N-3: \", bank_predictions[1][-3])\n",
    "print(\"\\nPredicted class probabilities for instance N-2: \", bank_predictions[0][-2])\n",
    "print(\"Predicted class for instance N-2: \", bank_predictions[1][-2])\n",
    "print(\"\\nPredicted class probabilities for instance N-1: \", bank_predictions[0][-1])\n",
    "print(\"Predicted class for instance N-1: \", bank_predictions[1][-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.48382126348228044\n",
      "Precision (macro avg):  0.48358500654751285\n",
      "Precision (micro avg):  0.48382126348228044\n",
      "Precision (weighted avg):  0.4857742081328605\n",
      "Recall (macro avg):  0.4763644469373179\n",
      "Recall (micro avg):  0.48382126348228044\n",
      "Recall (weighted avg):  0.48382126348228044\n",
      "F-score (macro avg):  0.4784911527273963\n",
      "F-score (micro avg):  0.48382126348228044\n",
      "F-score (weighted avg):  0.4834963990558278\n",
      "\n",
      "Feature vectors of instances [0, 1, 2]: \n",
      " [['GP' 'F' 'U' 'GT3' 'A' 'high' 'high' 'at_home' 'teacher' 'course'\n",
      "  'mother' 'medium' 'medium' 'none' 'yes' 'no' 'no' 'no' 'yes' 'yes' 'no'\n",
      "  'no' 'good' 'mediocre' 'good' 'very_bad' 'very_bad' 'mediocre'\n",
      "  'four_to_six']\n",
      " ['GP' 'F' 'U' 'GT3' 'T' 'low' 'low' 'at_home' 'other' 'course' 'father'\n",
      "  'low' 'medium' 'none' 'no' 'yes' 'no' 'no' 'no' 'yes' 'yes' 'no'\n",
      "  'excellent' 'mediocre' 'mediocre' 'very_bad' 'very_bad' 'mediocre'\n",
      "  'one_to_three']\n",
      " ['GP' 'F' 'U' 'LE3' 'T' 'low' 'low' 'at_home' 'other' 'other' 'mother'\n",
      "  'low' 'medium' 'none' 'yes' 'no' 'no' 'no' 'yes' 'yes' 'yes' 'no'\n",
      "  'good' 'mediocre' 'bad' 'bad' 'mediocre' 'mediocre' 'four_to_six']]\n",
      "\n",
      "Number of instances (N):  649\n",
      "Number of features (F):  29\n",
      "Number of labels (L):  6\n",
      "\n",
      "\n",
      "Predicted class probabilities for instance N-3:  {'D': -27.55235971394905, 'C': -29.09025010924399, 'B': -29.638141637141143, 'A': -38.82883602429322, 'F': -26.456204229824664, 'A+': -41.555440883743834}\n",
      "Predicted class for instance N-3:  F\n",
      "\n",
      "Predicted class probabilities for instance N-2:  {'D': -29.404207192607167, 'C': -35.384503684268765, 'B': -34.597061531595976, 'A': -38.26191454141717, 'F': -31.409841590368487, 'A+': -47.30539152048512}\n",
      "Predicted class for instance N-2:  D\n",
      "\n",
      "Predicted class probabilities for instance N-1:  {'D': -27.45157911437387, 'C': -33.42681333025393, 'B': -32.56646829531062, 'A': -40.91000879341258, 'F': -29.636481651604857, 'A+': -45.35502702108042}\n",
      "Predicted class for instance N-1:  D\n"
     ]
    }
   ],
   "source": [
    "# This cell should act as your \"main\" function where you call the above functions \n",
    "# on the full Student data set, and print the evaluation score. [0.33 marks]\n",
    "\n",
    "# First, read in the data and apply your NB model to the Student data\n",
    "student_data = preprocess('student.csv')\n",
    "student_model = train(student_data)\n",
    "student_predictions = predict(student_model, student_data)\n",
    "\n",
    "# Second, print the full evaluation results from the evaluate() function\n",
    "# This is the true labels of the dataset - to be used to evaluate the algorithm\n",
    "student_truth = list(student_data['label'])\n",
    "\n",
    "# ACCURACY SCORE\n",
    "student_accuracy =  evaluate(\"accuracy\", student_truth, student_predictions[1], pos_label=None, average=None)\n",
    "print(\"Accuracy: \", student_accuracy)\n",
    "\n",
    "# PRECISION\n",
    "student_precision_macro = evaluate(\"precision\", student_truth, student_predictions[1], pos_label=None, average=\"macro\")\n",
    "student_precision_micro = evaluate(\"precision\", student_truth, student_predictions[1], pos_label=None, average=\"micro\")\n",
    "student_precision_wavg = evaluate(\"precision\", student_truth, student_predictions[1], pos_label=None, average=\"weighted\")\n",
    "print(\"Precision (macro avg): \", student_precision_macro)\n",
    "print(\"Precision (micro avg): \", student_precision_micro)\n",
    "print(\"Precision (weighted avg): \", student_precision_wavg)\n",
    "\n",
    "# RECALL\n",
    "student_recall_macro = evaluate(\"recall\", student_truth, student_predictions[1], pos_label=None, average=\"macro\")\n",
    "student_recall_micro = evaluate(\"recall\", student_truth, student_predictions[1], pos_label=None, average=\"micro\")\n",
    "student_recall_wavg = evaluate(\"recall\", student_truth, student_predictions[1], pos_label=None, average=\"weighted\")\n",
    "print(\"Recall (macro avg): \", student_recall_macro)\n",
    "print(\"Recall (micro avg): \", student_recall_micro)\n",
    "print(\"Recall (weighted avg): \", student_recall_wavg)\n",
    "\n",
    "# F-SCORE\n",
    "student_fscore_macro = evaluate(\"f-score\", student_truth, student_predictions[1], pos_label=None, average=\"macro\")\n",
    "student_fscore_micro = evaluate(\"f-score\", student_truth, student_predictions[1], pos_label=None, average=\"micro\")\n",
    "student_fscore_wavg = evaluate(\"f-score\", student_truth, student_predictions[1], pos_label=None, average=\"weighted\")\n",
    "print(\"F-score (macro avg): \", student_fscore_macro)\n",
    "print(\"F-score (micro avg): \", student_fscore_micro)\n",
    "print(\"F-score (weighted avg): \", student_fscore_wavg)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "# Third, print data statistics and model predictions, as instructed below \n",
    "# N is the total number of instances, F the total number of features, L the total number of labels\n",
    "# The \"class probabilities\" may be unnormalized\n",
    "\n",
    "print(\"Feature vectors of instances [0, 1, 2]: \\n\", student_data.iloc[:,:-1].values[:3])\n",
    "\n",
    "print(\"\\nNumber of instances (N): \", student_data.shape[0])\n",
    "print(\"Number of features (F): \", len(list(student_data.columns[:-1])))\n",
    "print(\"Number of labels (L): \", len(list(student_data[\"label\"].unique())))\n",
    "\n",
    "print(\"\\n\\nPredicted class probabilities for instance N-3: \", student_predictions[0][-3])\n",
    "print(\"Predicted class for instance N-3: \", student_predictions[1][-3])\n",
    "print(\"\\nPredicted class probabilities for instance N-2: \", student_predictions[0][-2])\n",
    "print(\"Predicted class for instance N-2: \", student_predictions[1][-2])\n",
    "print(\"\\nPredicted class probabilities for instance N-1: \", student_predictions[0][-1])\n",
    "print(\"Predicted class for instance N-1: \", student_predictions[1][-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obesity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7783041212695405\n",
      "Precision (obese):  0.71\n",
      "Precision (not-obese):  0.8682766190998902\n",
      "Recall (obese):  0.8765432098765432\n",
      "Recall (not-obese):  0.694468832309043\n",
      "F-score (obese):  0.7845303867403315\n",
      "F-score (not-obese):  0.7717073170731708\n",
      "\n",
      "Feature vectors of instances [0, 1, 2]: \n",
      " [['Male' 'yes' 'yes' 'mid' 'high' 'Sometimes' 'yes' 'mid' 'no'\n",
      "  'low-activity' 'mediocre' 'Frequently' 'Public_Transportation']\n",
      " ['Male' 'yes' 'yes' 'mid' 'high' 'Sometimes' 'no' 'high' 'no'\n",
      "  'low-activity' 'good' 'Sometimes' 'Public_Transportation']\n",
      " ['Male' 'yes' 'yes' 'high' 'high' 'Sometimes' 'no' 'high' 'no'\n",
      "  'low-activity' 'good' 'Sometimes' 'Public_Transportation']]\n",
      "\n",
      "Number of instances (N):  2111\n",
      "Number of features (F):  13\n",
      "Number of labels (L):  2\n",
      "\n",
      "\n",
      "Predicted class probabilities for instance N-3:  {'not-obese': -11.010971850189438, 'obese': -16.78139621234939}\n",
      "Predicted class for instance N-3:  not-obese\n",
      "\n",
      "Predicted class probabilities for instance N-2:  {'not-obese': -10.299211743895615, 'obese': -15.249446788238835}\n",
      "Predicted class for instance N-2:  not-obese\n",
      "\n",
      "Predicted class probabilities for instance N-1:  {'not-obese': -7.261582154381628, 'obese': -5.340573755268082}\n",
      "Predicted class for instance N-1:  obese\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# This cell should act as your \"main\" function where you call the above functions \n",
    "# on the full Obesity data set, and print the evaluation score. [0.33 marks]\n",
    "\n",
    "# First, read in the data and apply your NB model to the Obesity data\n",
    "obesity_data = preprocess('obesity.csv')\n",
    "obesity_model = train(obesity_data)\n",
    "obesity_predictions = predict(obesity_model, obesity_data)\n",
    "\n",
    "# Second, print the full evaluation results from the evaluate() function\n",
    "# This is the true labels of the dataset - to be used to evaluate the algorithm\n",
    "obesity_truth = list(obesity_data['label'])\n",
    "\n",
    "# ACCURACY SCORE\n",
    "obesity_accuracy =  evaluate(\"accuracy\", obesity_truth, obesity_predictions[1], pos_label=None, average=None)\n",
    "print(\"Accuracy: \", obesity_accuracy)\n",
    "\n",
    "# PRECISION\n",
    "obesity_precision_yes =  evaluate(\"precision\", obesity_truth, obesity_predictions[1], pos_label=\"obese\", average=None)\n",
    "obesity_precision_no = evaluate(\"precision\", obesity_truth, obesity_predictions[1], pos_label=\"not-obese\", average=None)\n",
    "print(\"Precision (obese): \", obesity_precision_yes)\n",
    "print(\"Precision (not-obese): \", obesity_precision_no)\n",
    "\n",
    "# RECALL\n",
    "obesity_recall_yes =  evaluate(\"recall\", obesity_truth, obesity_predictions[1], pos_label=\"obese\", average=None)\n",
    "obesity_recall_no = evaluate(\"recall\", obesity_truth, obesity_predictions[1], pos_label=\"not-obese\", average=None)\n",
    "print(\"Recall (obese): \", obesity_recall_yes)\n",
    "print(\"Recall (not-obese): \", obesity_recall_no)\n",
    "\n",
    "# F-SCORE\n",
    "obesity_fscore_yes =  evaluate(\"f-score\", obesity_truth, obesity_predictions[1], pos_label=\"obese\", average=None)\n",
    "obesity_fscore_no = evaluate(\"f-score\", obesity_truth, obesity_predictions[1], pos_label=\"not-obese\", average=None)\n",
    "print(\"F-score (obese): \", obesity_fscore_yes)\n",
    "print(\"F-score (not-obese): \", obesity_fscore_no)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "# Third, print data statistics and model predictions, as instructed below \n",
    "# N is the total number of instances, F the total number of features, L the total number of labels\n",
    "# The \"class probabilities\" may be unnormalized\n",
    "\n",
    "print(\"Feature vectors of instances [0, 1, 2]: \\n\", obesity_data.iloc[:,:-1].values[:3])\n",
    "\n",
    "print(\"\\nNumber of instances (N): \", obesity_data.shape[0])\n",
    "print(\"Number of features (F): \", len(list(obesity_data.columns[:-1])))\n",
    "print(\"Number of labels (L): \", len(list(obesity_data[\"label\"].unique())))\n",
    "\n",
    "print(\"\\n\\nPredicted class probabilities for instance N-3: \", obesity_predictions[0][-3])\n",
    "print(\"Predicted class for instance N-3: \", obesity_predictions[1][-3])\n",
    "print(\"\\nPredicted class probabilities for instance N-2: \", obesity_predictions[0][-2])\n",
    "print(\"Predicted class for instance N-2: \", obesity_predictions[1][-2])\n",
    "print(\"\\nPredicted class probabilities for instance N-1: \", obesity_predictions[0][-1])\n",
    "print(\"Predicted class for instance N-1: \", obesity_predictions[1][-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2: Conceptual questions [13 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: One-R Baseline [3 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write additional code here, if necessary (you may insert additional code cells)\n",
    "# You should implement the One-R classifier from scratch. Do not use existing implementations/learning algorithms.\n",
    "# Print the feature name and its corresponding error rate that One-R selects, in addition to any evaluation scores.\n",
    "\n",
    "###### The 'TRAINING' function\n",
    "def one_r_baseline(data):\n",
    "    \n",
    "    # Get the list of features\n",
    "    R_features = list(data.columns[:-1])\n",
    "\n",
    "    # Set up dictionaries to hold the data    \n",
    "    feature_value_predictions = dict()\n",
    "    error_rates = dict()\n",
    "\n",
    "    # For each feature in the set of features\n",
    "    for ft in R_features:\n",
    "        \n",
    "        wrong_predictions = 0\n",
    "        total_predictions = 0\n",
    "        feature_value_predictions[ft] = {}\n",
    "\n",
    "        # get the list of values that each feature can take\n",
    "        feature_values = list(data[ft].unique())\n",
    "        \n",
    "        # for each value that the feature has \n",
    "        for ft_val in feature_values:\n",
    "\n",
    "            # find instances where the feature takes on the value\n",
    "            filtered_rows = data.loc[data[ft] == ft_val]\n",
    "\n",
    "            # find the counts of each class label\n",
    "            label_count = filtered_rows['label'].value_counts().to_dict()\n",
    "\n",
    "            # set the most frequent class as the prediction ---> 0th index since it is sorted by default\n",
    "            prediction = list(label_count)[0]\n",
    "\n",
    "            #append information to the dictionary\n",
    "            feature_value_predictions[ft][ft_val] = prediction\n",
    "\n",
    "            # calculate the correct / incorrect predictions for this specific feature-value pair \n",
    "            sum_val = sum(label_count.values())\n",
    "            total_predictions += sum_val\n",
    "\n",
    "            err_val = sum_val - label_count[prediction]\n",
    "            wrong_predictions += err_val\n",
    "    \n",
    "        # ERROR RATE CALCULATION \n",
    "        error_rt = wrong_predictions/total_predictions\n",
    "        error_rates[ft] = error_rt\n",
    "\n",
    "    # only return the feature-value set and error-rate of the 'best' (most predictive) set\n",
    "    error_rates = sorted(error_rates.items(), key=lambda x:x[1]) # sort the dictionary by error-rate in ascending order\n",
    "    most_predictive_feature_set = feature_value_predictions[error_rates[0][0]] # find the feature set that provides the best prediction of the class label\n",
    "    most_predictive_feature_set = [(k, v) for k, v in most_predictive_feature_set.items()] #convert dict to list of tuples - https://www.geeksforgeeks.org/python-convert-dictionary-to-list-of-tuples/\n",
    "\n",
    "    return error_rates[0], most_predictive_feature_set\n",
    "\n",
    "\n",
    "##### The 'PREDICTION' function\n",
    "def one_r_predict(oneR_data, base_data):\n",
    "    \n",
    "    feature_name = oneR_data[0][0]\n",
    "    prediction_guide = oneR_data[1]\n",
    "\n",
    "    # create a copy of the dataset so that we can safely manipulate it\n",
    "    data_copy = base_data\n",
    "\n",
    "    for value in prediction_guide:\n",
    "        \n",
    "        feature_val = value[0]\n",
    "        prediction = value[1]\n",
    "        \n",
    "        # Change class based on the prediction guide for the chosen feature -- https://kanoki.org/2019/07/17/pandas-how-to-replace-values-based-on-conditions/\n",
    "        data_copy.loc[(data_copy[feature_name] == feature_val), 'label']=prediction\n",
    "    \n",
    "    # extract the label predictions into a list to be used for evaluation\n",
    "    oneR_predictions = data_copy['label'].tolist()\n",
    "    \n",
    "    return oneR_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== One-R insights from the BANK MARKETING dataset =====\n",
      "For the BANK MARKETING dataset --- the most predictive feature and its corresponding error rate is: ('poutcome', 0.1070559610705596)\n",
      "\n",
      "Accuracy:  0.8929440389294404\n",
      "Precision (yes):  0.6434108527131783\n",
      "Precision (no):  0.9002732240437158\n",
      "Recall (yes):  0.15930902111324377\n",
      "Recall (no):  0.9885\n",
      "F-score (yes):  0.2553846153846154\n",
      "F-score (no):  0.94232602478551\n",
      "\n",
      "\n",
      "===== One-R insights from the STUDENT dataset =====\n",
      "For the STUDENT dataset --- the most predictive feature and its corresponding error rate is: ('Fedu', 0.6656394453004623)\n",
      "\n",
      "Accuracy:  0.33436055469953774\n",
      "Precision (macro avg):  0.10674084293026231\n",
      "Precision (micro avg):  0.33436055469953774\n",
      "Precision (weighted avg):  0.17685105019090347\n",
      "Recall (macro avg):  0.18955008507247315\n",
      "Recall (micro avg):  0.33436055469953774\n",
      "Recall (weighted avg):  0.33436055469953774\n",
      "F-score (macro avg):  0.1275580702409252\n",
      "F-score (micro avg):  0.33436055469953774\n",
      "F-score (weighted avg):  0.2175167427486735\n",
      "\n",
      "\n",
      "===== One-R insights from the OBESITY dataset =====\n",
      "For the OBESITY dataset --- the most predictive feature and its corresponding error rate is: ('family_history_with_overweight', 0.36475603979156795)\n",
      "\n",
      "Accuracy:  0.635243960208432\n",
      "Precision (obese):  0.5585168018539977\n",
      "Precision (not-obese):  0.9792207792207792\n",
      "Recall (obese):  0.9917695473251029\n",
      "Recall (not-obese):  0.33099209833187004\n",
      "F-score (obese):  0.7146034099332839\n",
      "F-score (not-obese):  0.4947506561679791\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# BANKING ################################################################################################################################\n",
    "print(\"===== One-R insights from the BANK MARKETING dataset =====\")\n",
    "oneR_bank_data = preprocess(\"bank-marketing.csv\")\n",
    "oneR_bank_model = one_r_baseline(oneR_bank_data)\n",
    "oneR_bank_predict = one_r_predict(oneR_bank_model, oneR_bank_data)\n",
    "\n",
    "print(\"For the BANK MARKETING dataset --- the most predictive feature and its corresponding error rate is:\", oneR_bank_model[0])\n",
    "print(\"\")\n",
    "\n",
    "# ACCURACY SCORE\n",
    "oneR_bank_accuracy =  evaluate(\"accuracy\", bank_truth, oneR_bank_predict, pos_label=None, average=None)\n",
    "print(\"Accuracy: \", oneR_bank_accuracy)\n",
    "\n",
    "# PRECISION\n",
    "oneR_bank_precision_yes =  evaluate(\"precision\", bank_truth, oneR_bank_predict, pos_label=\"yes\", average=None)\n",
    "oneR_bank_precision_no = evaluate(\"precision\", bank_truth, oneR_bank_predict, pos_label=\"no\", average=None)\n",
    "print(\"Precision (yes): \", oneR_bank_precision_yes)\n",
    "print(\"Precision (no): \", oneR_bank_precision_no)\n",
    "\n",
    "# RECALL\n",
    "oneR_bank_recall_yes =  evaluate(\"recall\", bank_truth, oneR_bank_predict, pos_label=\"yes\", average=None)\n",
    "oneR_bank_recall_no = evaluate(\"recall\", bank_truth, oneR_bank_predict, pos_label=\"no\", average=None)\n",
    "print(\"Recall (yes): \", oneR_bank_recall_yes)\n",
    "print(\"Recall (no): \", oneR_bank_recall_no)\n",
    "\n",
    "# F-SCORE\n",
    "oneR_bank_fscore_yes =  evaluate(\"f-score\", bank_truth, oneR_bank_predict, pos_label=\"yes\", average=None)\n",
    "oneR_bank_fscore_no = evaluate(\"f-score\", bank_truth, oneR_bank_predict, pos_label=\"no\", average=None)\n",
    "print(\"F-score (yes): \", oneR_bank_fscore_yes)\n",
    "print(\"F-score (no): \", oneR_bank_fscore_no)\n",
    "\n",
    "\n",
    "\n",
    "# STUDENT ################################################################################################################################\n",
    "print(\"\\n\\n===== One-R insights from the STUDENT dataset =====\")\n",
    "oneR_student_data = preprocess(\"student.csv\")\n",
    "oneR_student_model = one_r_baseline(oneR_student_data)\n",
    "oneR_student_predict = one_r_predict(oneR_student_model, oneR_student_data)\n",
    "\n",
    "print(\"For the STUDENT dataset --- the most predictive feature and its corresponding error rate is:\", oneR_student_model[0])\n",
    "print(\"\")\n",
    "\n",
    "# ACCURACY SCORE\n",
    "oneR_student_accuracy =  evaluate(\"accuracy\", student_truth, oneR_student_predict, pos_label=None, average=None)\n",
    "print(\"Accuracy: \", oneR_student_accuracy)\n",
    "\n",
    "# PRECISION\n",
    "oneR_student_precision_macro = evaluate(\"precision\", student_truth, oneR_student_predict, pos_label=None, average=\"macro\")\n",
    "oneR_student_precision_micro = evaluate(\"precision\", student_truth, oneR_student_predict, pos_label=None, average=\"micro\")\n",
    "oneR_student_precision_wavg = evaluate(\"precision\", student_truth, oneR_student_predict, pos_label=None, average=\"weighted\")\n",
    "print(\"Precision (macro avg): \", oneR_student_precision_macro)\n",
    "print(\"Precision (micro avg): \", oneR_student_precision_micro)\n",
    "print(\"Precision (weighted avg): \", oneR_student_precision_wavg)\n",
    "\n",
    "# RECALL\n",
    "oneR_student_recall_macro = evaluate(\"recall\", student_truth, oneR_student_predict, pos_label=None, average=\"macro\")\n",
    "oneR_student_recall_micro = evaluate(\"recall\", student_truth, oneR_student_predict, pos_label=None, average=\"micro\")\n",
    "oneR_student_recall_wavg = evaluate(\"recall\", student_truth, oneR_student_predict, pos_label=None, average=\"weighted\")\n",
    "print(\"Recall (macro avg): \", oneR_student_recall_macro)\n",
    "print(\"Recall (micro avg): \", oneR_student_recall_micro)\n",
    "print(\"Recall (weighted avg): \", oneR_student_recall_wavg)\n",
    "\n",
    "# F-SCORE\n",
    "oneR_student_fscore_macro = evaluate(\"f-score\", student_truth, oneR_student_predict, pos_label=None, average=\"macro\")\n",
    "oneR_student_fscore_micro = evaluate(\"f-score\", student_truth, oneR_student_predict, pos_label=None, average=\"micro\")\n",
    "oneR_student_fscore_wavg = evaluate(\"f-score\", student_truth, oneR_student_predict, pos_label=None, average=\"weighted\")\n",
    "print(\"F-score (macro avg): \", oneR_student_fscore_macro)\n",
    "print(\"F-score (micro avg): \", oneR_student_fscore_micro)\n",
    "print(\"F-score (weighted avg): \", oneR_student_fscore_wavg)\n",
    "\n",
    "\n",
    "\n",
    "# OBESITY ################################################################################################################################\n",
    "print(\"\\n\\n===== One-R insights from the OBESITY dataset =====\")\n",
    "oneR_obesity_data = preprocess(\"obesity.csv\")\n",
    "oneR_obesity_model = one_r_baseline(oneR_obesity_data)\n",
    "oneR_obesity_predict = one_r_predict(oneR_obesity_model, oneR_obesity_data)\n",
    "\n",
    "print(\"For the OBESITY dataset --- the most predictive feature and its corresponding error rate is:\", oneR_obesity_model[0])\n",
    "print(\"\")\n",
    "\n",
    "# ACCURACY SCORE\n",
    "oneR_obesity_accuracy =  evaluate(\"accuracy\", obesity_truth, oneR_obesity_predict, pos_label=None, average=None)\n",
    "print(\"Accuracy: \", oneR_obesity_accuracy)\n",
    "\n",
    "# PRECISION\n",
    "oneR_obesity_precision_yes =  evaluate(\"precision\", obesity_truth, oneR_obesity_predict, pos_label=\"obese\", average=None)\n",
    "oneR_obesity_precision_no = evaluate(\"precision\", obesity_truth, oneR_obesity_predict, pos_label=\"not-obese\", average=None)\n",
    "print(\"Precision (obese): \", oneR_obesity_precision_yes)\n",
    "print(\"Precision (not-obese): \", oneR_obesity_precision_no)\n",
    "\n",
    "# RECALL\n",
    "oneR_obesity_recall_yes =  evaluate(\"recall\", obesity_truth, oneR_obesity_predict, pos_label=\"obese\", average=None)\n",
    "oneR_obesity_recall_no = evaluate(\"recall\", obesity_truth, oneR_obesity_predict, pos_label=\"not-obese\", average=None)\n",
    "print(\"Recall (obese): \", oneR_obesity_recall_yes)\n",
    "print(\"Recall (not-obese): \", oneR_obesity_recall_no)\n",
    "\n",
    "# F-SCORE\n",
    "oneR_obesity_fscore_yes =  evaluate(\"f-score\", obesity_truth, oneR_obesity_predict, pos_label=\"obese\", average=None)\n",
    "oneR_obesity_fscore_no = evaluate(\"f-score\", obesity_truth, oneR_obesity_predict, pos_label=\"not-obese\", average=None)\n",
    "print(\"F-score (obese): \", oneR_obesity_fscore_yes)\n",
    "print(\"F-score (not-obese): \", oneR_obesity_fscore_no)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.b**\n",
    "\n",
    "For the student dataset, the Naive Bayes classifier (NB) is clearly the superior classifier. NB shows an accuracy rating of 0.484; meaning 48.4% of the predictions made were correct. In contrast, One-R baseline (OR) only managed to predict the correct class 33.4% of the time. Furthermore, the figures for precision, recall and f-score using NB all hover around 48%, while for OR it is around 10-30%. This is fairly logical as the student dataset is multiclass with much more nuance that is lost when using a single rule for prediction based only on frequency.\n",
    "\n",
    "For the bank-marketing dataset, we see the opposite behaviour. OR makes an accurate prediction 89.3% of the time, while NB correctly predicts 88.5% of the time. The figures for precision, recall and f-score for NB and OR are also similar. A likely explanation is that this dataset is extremely weighted (No: 88.5%, Yes: 11.5%). This means we can be very accurate by simple classifying all instances as \"No\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Evaluation strategy [3 marks] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write additional code here, if necessary (you may insert additional code cells)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def kf_cv(n, raw_data, classification_type, int_label, not_int_label):\n",
    "\n",
    "    kf = KFold(n_splits=n, random_state=None, shuffle=False)\n",
    "    \n",
    "    accuracy_scores = []\n",
    "    \n",
    "    # Binary dataset\n",
    "    precision_interesting = []\n",
    "    precision_not_interesting = []\n",
    "    recall_interesting = []\n",
    "    recall_not_interesting = []\n",
    "    fscore_interesting = []\n",
    "    fscore_not_interesting = []\n",
    "\n",
    "    # Multiclass dataset\n",
    "    precision_macro = []\n",
    "    precision_micro = []\n",
    "    precision_wavg = []\n",
    "    recall_macro = []\n",
    "    recall_micro = []\n",
    "    recall_wavg = []\n",
    "    fscore_macro = []\n",
    "    fscore_micro = []\n",
    "    fscore_wavg = []\n",
    "\n",
    "\n",
    "    for train_index, test_index in kf.split(raw_data):\n",
    "\n",
    "        # Train data\n",
    "        train_data = raw_data.iloc[train_index,:]\n",
    "\n",
    "        # Test data\n",
    "        test_data = raw_data.iloc[test_index, :]\n",
    "        \n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # Train the model\n",
    "        cv_train_model = train(train_data)\n",
    "\n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # Predict classes for the test model\n",
    "        cv_prediction = predict(cv_train_model, test_data)\n",
    "        \n",
    "        # -----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # Evaluate the model\n",
    "        truth = list(test_data[\"label\"])\n",
    "\n",
    "        # ACCURACY\n",
    "        acc = evaluate(\"accuracy\", truth, cv_prediction[1], pos_label=None, average=None)\n",
    "        accuracy_scores.append(acc)\n",
    "        \n",
    "        # Type of evaluation depends on whether we are performing binary or multiclass predictions\n",
    "        if classification_type == \"binary\": \n",
    "            \n",
    "            # PRECISION\n",
    "            p_int = evaluate(\"precision\", truth, cv_prediction[1], pos_label=int_label, average=None)\n",
    "            p_not_int = evaluate(\"precision\", truth, cv_prediction[1], pos_label=not_int_label, average=None)\n",
    "            precision_interesting.append(p_int)\n",
    "            precision_not_interesting.append(p_not_int)\n",
    "            \n",
    "            # RECALL\n",
    "            r_int = evaluate(\"recall\", truth, cv_prediction[1], pos_label=int_label, average=None)\n",
    "            r_not_int = evaluate(\"recall\", truth, cv_prediction[1], pos_label=not_int_label, average=None)\n",
    "            recall_interesting.append(r_int)\n",
    "            recall_not_interesting.append(r_not_int)\n",
    "\n",
    "            # FSCORE\n",
    "            f_int = evaluate(\"f-score\", truth, cv_prediction[1], pos_label=int_label, average=None)\n",
    "            f_not_int = evaluate(\"f-score\", truth, cv_prediction[1], pos_label=not_int_label, average=None)\n",
    "            fscore_interesting.append(f_int)\n",
    "            fscore_not_interesting.append(f_not_int)\n",
    "\n",
    "        if classification_type == \"multiclass\":\n",
    "\n",
    "            # PRECISION\n",
    "            p_macro = evaluate(\"precision\", truth, cv_prediction[1], pos_label=None, average=\"macro\")\n",
    "            p_micro = evaluate(\"precision\", truth, cv_prediction[1], pos_label=None, average=\"micro\")\n",
    "            p_wavg = evaluate(\"precision\", truth, cv_prediction[1], pos_label=None, average=\"weighted\")\n",
    "            precision_macro.append(p_macro)\n",
    "            precision_micro.append(p_micro)\n",
    "            precision_wavg.append(p_wavg)\n",
    "\n",
    "            # RECALL\n",
    "            r_macro = evaluate(\"recall\", truth, cv_prediction[1], pos_label=None, average=\"macro\")\n",
    "            r_micro = evaluate(\"recall\", truth, cv_prediction[1], pos_label=None, average=\"micro\")\n",
    "            r_wavg = evaluate(\"recall\", truth, cv_prediction[1], pos_label=None, average=\"weighted\")\n",
    "            recall_macro.append(p_macro)\n",
    "            recall_micro.append(p_micro)\n",
    "            recall_wavg.append(p_wavg)\n",
    "\n",
    "            # FSCORE\n",
    "            f_macro = evaluate(\"f-score\", truth, cv_prediction[1], pos_label=None, average=\"macro\")\n",
    "            f_micro = evaluate(\"f-score\", truth, cv_prediction[1], pos_label=None, average=\"micro\")\n",
    "            f_wavg = evaluate(\"f-score\", truth, cv_prediction[1], pos_label=None, average=\"weighted\")\n",
    "            fscore_macro.append(p_macro)\n",
    "            fscore_micro.append(p_micro)\n",
    "            fscore_wavg.append(p_wavg)\n",
    "\n",
    " # -----------------------------------------------------------------------------------------------------------\n",
    "    # print the evaluation information\n",
    "    if classification_type == \"binary\": \n",
    "        print(\"Evaluation scores for \" + str(n) + \" fold crossvalidation (averaged): \")\n",
    "        print(\"     Accuracy: \", np.mean(accuracy_scores))\n",
    "        print(\"     Precision (interesting): \", np.mean(precision_interesting))\n",
    "        print(\"     Precision (not interesting): \", np.mean(precision_not_interesting))\n",
    "        print(\"     Recall (interesting): \", np.mean(recall_interesting))\n",
    "        print(\"     Recall (not interesting): \", np.mean(recall_not_interesting))\n",
    "        print(\"     F-score (interesting): \", np.mean(fscore_interesting))\n",
    "        print(\"     F-score (not interesting): \", np.mean(fscore_not_interesting))\n",
    "\n",
    "    if classification_type == \"multiclass\": \n",
    "        print(\"Evaluation scores for \" + str(n) + \" fold crossvalidation (averaged): \")\n",
    "        print(\"     Accuracy: \", np.mean(accuracy_scores))\n",
    "        print(\"     Precision (macro avg): \", np.mean(precision_macro))\n",
    "        print(\"     Precision (micro avg): \", np.mean(precision_micro))\n",
    "        print(\"     Precision (weighted avg): \", np.mean(precision_wavg))\n",
    "        print(\"     Recall (macro avg): \", np.mean(recall_macro))\n",
    "        print(\"     Recall (micro avg): \", np.mean(recall_micro))\n",
    "        print(\"     Recall (weighted avg): \", np.mean(recall_wavg))\n",
    "        print(\"     F-score (macro avg): \", np.mean(fscore_macro))\n",
    "        print(\"     F-score (micro avg): \", np.mean(fscore_micro))\n",
    "        print(\"     F-score (weighted avg): \", np.mean(fscore_wavg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Fold Cross Validation for the BANK-MARKETING dataset (k=2, k=10) ------------------------------- \n",
      "\n",
      "Evaluation scores for 2 fold crossvalidation (averaged): \n",
      "     Accuracy:  0.8865311378393929\n",
      "     Precision (interesting):  0.5207784606379866\n",
      "     Precision (not interesting):  0.9068610342422092\n",
      "     Recall (interesting):  0.23377121771217713\n",
      "     Recall (not interesting):  0.9714717867946698\n",
      "     F-score (interesting):  0.32205387205387204\n",
      "     F-score (not interesting):  0.9380540029473785\n",
      "\n",
      "Evaluation scores for 10 fold crossvalidation (averaged): \n",
      "     Accuracy:  0.8863100470804275\n",
      "     Precision (interesting):  0.516582467875374\n",
      "     Precision (not interesting):  0.9072458881543597\n",
      "     Recall (interesting):  0.23971910948253305\n",
      "     Recall (not interesting):  0.9707688841103888\n",
      "     F-score (interesting):  0.32578760282617936\n",
      "     F-score (not interesting):  0.937867872257641\n",
      "\n",
      "\n",
      "K-Fold Cross Validation for the STUDENT dataset (k=2, k=10) ------------------------------- \n",
      "\n",
      "Evaluation scores for 2 fold crossvalidation (averaged): \n",
      "     Accuracy:  0.309696106362773\n",
      "     Precision (macro avg):  0.2746114430783236\n",
      "     Precision (micro avg):  0.309696106362773\n",
      "     Precision (weighted avg):  0.34329119454860113\n",
      "     Recall (macro avg):  0.2746114430783236\n",
      "     Recall (micro avg):  0.309696106362773\n",
      "     Recall (weighted avg):  0.34329119454860113\n",
      "     F-score (macro avg):  0.2746114430783236\n",
      "     F-score (micro avg):  0.309696106362773\n",
      "     F-score (weighted avg):  0.34329119454860113\n",
      "\n",
      "Evaluation scores for 10 fold crossvalidation (averaged): \n",
      "     Accuracy:  0.35742788461538466\n",
      "     Precision (macro avg):  0.29483854524012754\n",
      "     Precision (micro avg):  0.35742788461538466\n",
      "     Precision (weighted avg):  0.3711756683245625\n",
      "     Recall (macro avg):  0.29483854524012754\n",
      "     Recall (micro avg):  0.35742788461538466\n",
      "     Recall (weighted avg):  0.3711756683245625\n",
      "     F-score (macro avg):  0.29483854524012754\n",
      "     F-score (micro avg):  0.35742788461538466\n",
      "     F-score (weighted avg):  0.3711756683245625\n",
      "\n",
      "\n",
      "K-Fold Cross Validation for the OBESITY dataset (k=2, k=10) ------------------------------- \n",
      "\n",
      "Evaluation scores for 2 fold crossvalidation (averaged): \n",
      "     Accuracy:  0.7730858645698693\n",
      "     Precision (interesting):  0.708545684113866\n",
      "     Precision (not interesting):  0.8557763078105254\n",
      "     Recall (interesting):  0.861248475816285\n",
      "     Recall (not interesting):  0.6979748646415314\n",
      "     F-score (interesting):  0.7773117366418802\n",
      "     F-score (not interesting):  0.7686359057172603\n",
      "\n",
      "Evaluation scores for 10 fold crossvalidation (averaged): \n",
      "     Accuracy:  0.7778234820710006\n",
      "     Precision (interesting):  0.7104291074020526\n",
      "     Precision (not interesting):  0.8671581333151372\n",
      "     Recall (interesting):  0.8757222571316301\n",
      "     Recall (not interesting):  0.6967962608378518\n",
      "     F-score (interesting):  0.7831477694277376\n",
      "     F-score (not interesting):  0.7713174461055543\n"
     ]
    }
   ],
   "source": [
    "print(\"K-Fold Cross Validation for the BANK-MARKETING dataset (k=2, k=10) ------------------------------- \\n\")\n",
    "raw_bank = preprocess('bank-marketing.csv')\n",
    "kf_cv(2, raw_bank, \"binary\", \"yes\", \"no\")\n",
    "print(\"\")\n",
    "kf_cv(10, raw_bank, \"binary\", \"yes\", \"no\")\n",
    "\n",
    "print(\"\\n\\nK-Fold Cross Validation for the STUDENT dataset (k=2, k=10) ------------------------------- \\n\")\n",
    "raw_student = preprocess('student.csv')\n",
    "kf_cv(2, raw_student, \"multiclass\", None, None)\n",
    "print(\"\")\n",
    "kf_cv(10, raw_student, \"multiclass\", None, None)\n",
    "\n",
    "print(\"\\n\\nK-Fold Cross Validation for the OBESITY dataset (k=2, k=10) ------------------------------- \\n\")\n",
    "raw_obesity = preprocess('obesity.csv')\n",
    "kf_cv(2, raw_obesity, \"binary\", \"obese\", \"not-obese\")\n",
    "print(\"\")\n",
    "kf_cv(10, raw_obesity, \"binary\", \"obese\", \"not-obese\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2**\n",
    "\n",
    "I implemented a 2-fold and 10-fold cross validation (CV) strategy for each of the datasets. I observed lower evaluation scores for almost all of the measures (accuracy, precision, recall, f-score) when using CV, compared to those observed when training on the full dataset. Furthermore, 10-fold CV outperformed 2-fold CV for the 'student' and 'obesity' dataset, however the opposite behaviour was seen in the 'bank-marketing' dataset.\n",
    "\n",
    "Overall, this was expected, as CV never has any overlap in train/test datasets (even though it technically trains/tests over the whole set), and so dataset splits between each fold will produce slightly more variable results as the model is not seeing a portion of the data for each iteration. The effect of this is minimised as we average the evaluation metrics. \n",
    "\n",
    "In terms of the obesity and bank marketing dataset, the improvement in evaluation scores between testing on all train data vs. CV is extremely marginal (Accuracy scores --> [Bank_full: 0.8874, Bank_CV: 0.8865], [Obesity_full: 0.7783, Obesity_CV: 0.7778]). \n",
    "\n",
    "A likely explanation for 'bank-marketing' is that it is a binary classification dataset, with one of the labels heavily outweighing the other (88.5% of instances = No). A potential explanation for 'obesity' is that there is a feature-set which strongly predicts the data, and since this is observed in all instances, the predictions will always be similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Feature Selection and Naive Bayes Assumptions [3 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write additional code here, if necessary (you may insert additional code cells)\n",
    "# Resource used: https://www.kaggle.com/code/ryanholbrook/mutual-information/tutorial\n",
    "from sklearn.feature_selection import mutual_info_classif as mic\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_mutual_information(data):\n",
    "\n",
    "    mi_features = data.iloc[:,:-1]\n",
    "    mi_labels = data[\"label\"]\n",
    "\n",
    "    # Transform categorial data to numerical data - https://stackoverflow.com/questions/39390160/pandas-factorize-on-an-entire-data-frame/39390208#39390208\n",
    "    numerical_mi_features = mi_features.apply(lambda x: pd.factorize(x)[0])\n",
    "\n",
    "    # Calculate the mutual information scores\n",
    "    mi_score = pd.Series(mic(numerical_mi_features, mi_labels), index=numerical_mi_features.columns)\n",
    "    mi_score = mi_score.sort_values()\n",
    "    print(\"Feature  /  MI scores (ascending)\")\n",
    "    print(  mi_score)\n",
    "\n",
    "    # Plot the graph\n",
    "    plt.figure()\n",
    "    width = np.arange(len(mi_score))\n",
    "    ticks = list(mi_score.index)\n",
    "    plt.barh(width, mi_score)\n",
    "    plt.yticks(width, ticks)\n",
    "    plt.title(\"Mutual Information Scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MUTUAL INFORMATION FOR OBESITY ==================================\n",
      "Feature  /  MI scores (ascending)\n",
      "FCVC                              0.007941\n",
      "FAF                               0.008096\n",
      "TUE                               0.009024\n",
      "SMOKE                             0.009603\n",
      "Gender                            0.010850\n",
      "SCC                               0.018514\n",
      "CH2O                              0.020755\n",
      "MTRANS                            0.022947\n",
      "CALC                              0.029511\n",
      "FAVC                              0.043352\n",
      "NCP                               0.045909\n",
      "CAEC                              0.101170\n",
      "family_history_with_overweight    0.111248\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "MUTUAL INFORMATION FOR BANK-MARKETING ==================================\n",
      "Feature  /  MI scores (ascending)\n",
      "marital      0.000000\n",
      "education    0.000110\n",
      "default      0.001182\n",
      "job          0.006538\n",
      "housing      0.009318\n",
      "loan         0.009703\n",
      "contact      0.014261\n",
      "month        0.021924\n",
      "poutcome     0.022818\n",
      "dtype: float64\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgEAAAEICAYAAADGASc0AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAmZElEQVR4nO3deZwcVb338c+XQEJCSFDWEIFhCSABjBBAZBfkiiKrQhCEIBJwveANCsKjUbzCRZD9XgzIpkhY48Pmw3IhEjZhAjEboCxhCzsSlpBAkt/zR52GSqdnpnu6exbq+369+jVVp06dPqcnmfrVOafrKCIwMzOz4lmmuytgZmZm3cNBgJmZWUE5CDAzMysoBwFmZmYF5SDAzMysoBwEmJmZFZSDADPr8SS1SApJyzagrNUl3S3pbUlnNKJ+jSTpp5Iu6u56WDE4CDCzJUiaLel9SauUpU9NF+KWKssJSRs0pZJLv9dsSbtVmX0M8BowKCL+o4nV6pCknSU9n0+LiF9HxLeb8F59JZ0h6XlJ70h6WtKZjX4f610cBJhZJU8DB5V2JG0G9O++6jTUOsCs6MST0hrRE9GNTgBGAlsDKwK7AI808g16+edTSA4CzKySPwCH5vYPAy7PZ5A0SdK3c/ujJd2Ttu9OyX9Pd50H5o/nzvmwt0DSVyQ9IuktSc9JGteZipfeR9Lpkv6V7nj3SMcuTW35carXbpL6STpL0pz0OktSv5R/53Tn/BNJLwGXSBon6RpJf0xDCtMlbSjpBEmvpLrvnqvP4ZIeTXmfknRUSl8B+AuwZqrLO5LWTOX/MXf+XpJmSnozfeafzh2bLWmspGmS5kq6StLybXw0WwETI2JOZGZHxOW5staSdL2kVyW9Lum8lL6MpJMkPZPad7mkwelYaZjmCEnPAnem9G+lNv9L0q2S1knpknRmKmduqvemnfk9W2M4CDCzSh4ABkn6tKQ+wIHAHzs450MRsWPa/ExEDIyIq6o47V2ywGMl4CvAdyTtU1OtP7IN8DiwCnAa8HtJiojRwBXAaaledwAnAp8DRgCfIbtTPilX1hrAJ8l6EMaktK+SBUqfILubvpXs7+lQ4JfA73LnvwLsCQwCDgfOlLRFRLwL7AHMSXUZGBFz8o2QtCFwJXAMsCpwC3CjpL65bAcAXwLWBTYHRrfxmTwA/EjSdyVtJkm59+kD3AQ8A7SkdkxIh0en1y7AesBA4LyysncCPg38W/qd/RTYL9V5cmoDwO7AjsCGZL/nA4HX26ivdQEHAWbWllJvwBeBx4AXmvlmETEpIqZHxOKImEZ24dipk8U9ExEXRsQi4DJgCLB6G3kPBn4ZEa9ExKvAL4Bv5o4vBn4eEQsi4r2UNjkibo2IhcA1ZBe7UyPiA7KLZ4uklVK7bo6IJ9Pd91+B24AdqmzHgcDNEXF7Kvt0smGZz+fynJPu7t8AbiQLZio5Bfiv1N5W4AVJh6VjWwNrAsdFxLsRMT8iSr02BwO/jYinIuIdsmGFUWVd/+PSee8BRwGnRMSj6fP5NTAi9QZ8QDYUsTGglOfFKj8LawIHAWbWlj8A3yC7C7y8/az1k7SNpLtSd/Rc4GiyO/nOeKm0ERHz0ubANvKuSXYHXPJMSit5NSLml53zcm77PeC1FHCU9j98P0l7SHpA0huS3gS+TPXtWqJuEbEYeI7sTr3kpdz2PNpoZ0QsiojzI2I7srvw/wQuTsMLa5EFTgs7qkPaXpYlg6rnctvrAGen4Ys3gTcAAUMj4k6yXoTzgZcljZc0qI22WxdwEGBmFUXEM2QTBL8MXF8hy7vAgNz+Gh0UuUR+SeX5/wTcAKwVEYOBC8guHs02h+zCVbJ2Sivp9FKraW7BdWR38KtHxEpkXfqldnVU9hJ1S134a1Fnr0xEvBcR5wP/AjYhu4ivrcoT+yp9PgtZMhDKt+M54KiIWCn36h8R96X3PicitgSGkw0LHFdPW6w+DgLMrD1HAF9I49flpgL7SRqQJvcdUXb8ZbIx5JK/A8MljUiT18aV5V8ReCMi5kvamqwXoitcCZwkaVVlX4v8GTXMf+hAX6Af8CqwME1Q3D13/GVg5dJEuwquBr4iaVdJywH/ASwA7qu1IpKOSRMd+0taNg0FrEg2p+FB4EXgVEkrSFpe0nbp1CuBYyWtK2kgWff+VW30GkAWvJ0gaXh638GSvp62t0o9PsuRBYXzgUVtlGNdwEGAmbUpjWW3tnH4TOB9sgvZZWQT7vLGAZelbuEDIuIfZJPm7gD+CdxTlv+7wC8lvU12Ib66Ma3o0K/IxsinAdOBh1Na3SLibeCHZG35F1lgc0Pu+GNkF9mn0ue0Ztn5jwOHAOeSPdvgq8BXI+L9TlTnPeAMsuGD14DvAfunsf5FqewNgGeB58nmIwBcTDY0dDdZz9B84AfttHki2dyDCZLeAmaQTYCEbHLkhemzeIZsUuDpnWiLNYg68VVZMzMz+xhwT4CZmVlBOQgwMzMrKAcBZmZmBeUgwMzMrKC82IP1Kqusskq0tLR0dzXMzHqVKVOmvBYRq5anOwiwXqWlpYXW1ra+sWZmZpVIeqZSuocDzMzMCspBgJmZWUE5CDAzMysoBwFmZmYF5SDAzMysoBwEmJmZFZSDADMzs4JyEGBmZlZQfliQ9SrTX5hLy/E3d3c1zMy61OxTv9KUct0TYGZmVlAOAszMzArKQYCZmVlBdRgESPqhpEclXVHPG0n6paTd0vYkSSNrPL9F0oyOym7j+D6SNqmtxo0l6WhJh6bt0ZLWzB2bLWmV7qtdfSTdV0Weim2UtLOkzzenZmZm1p5qJgZ+F9gjIp6u540i4mf1nF9n2fsANwGzqi1T0rIRsbCeeuVFxAW53dHADGBOo8qvR71tjYh6LuI7A+8AHQYSZmbWWO32BEi6AFgPuEHSTyTdJ+mR9HOjlGe0pD9LulHS05K+L+lHKd8Dkj6Z8l0q6Wtl5R8h6czc/pGSfttOlfpIulDSTEm3SepfXrakUyXNkjRN0unpLnMv4DeSpkpaX9KIVLdpkiZK+kQ6d5KkX0v6K3Bias9y6digdDe7XIXPaTVJU9L2ZySFpLXT/pOSBkgaJ2lsqudI4IpUn/6pmB9IeljSdEkbt/M7+WT6vKelNmwuaZlUt5Vy+Z6QtLqkVSVdJ+mh9NouHR8nabyk24DLJd0iafN07BFJP0vbJ0v6dto+LpUxTdIvcu/1Tvq5jKT/Tr+fm1KZ+d/5Em2U1AIcDRybPosd2mjzGEmtkloXzZvb1kdjZmY1ajcIiIijye5WdwH+B9gxIj4L/Az4dS7rpsA3gK2B/wTmpXz3A4e28xYTgL1yF9bDgUvayT8MOD8ihgNvAvvnD6aAY19geERsDvwqIu4DbgCOi4gREfEkcDnwk5RnOvDzXDErRcROEfELYBJQ+l7GKOC6iPigvFIR8QqwvKRBwA5AK7CDpHWAVyJiXi7vten4wak+76VDr0XEFmSf89h2PoNfAI+kuv8UuDwiFgP/N7UdSdsAsyPiZeBs4MyI2Cp9XhflytoS2DsivgHcneo8CFgIbJfybA9MlrQ72ee/NTAC2FLSjmV12w9oATYDvg1sW3Z8iTZGxGzgglS/ERExuVKDI2J8RIyMiJF9Bgxu56MxM7Na1DIxcDBwjbJx+TOB4bljd0XE2xHxKjAXuDGlTye7KFQUEe8CdwJ7prvf5SJiejt1eDoipqbtKRXKfguYD1wkaT9gXtlxJA0mu9D/NSVdBuQvZlflti8iC0yg4wDlPrIL545kAdKOZAFBxQtbBdenn5Xalbc98AeAiLgTWDm16SrgwJRnVK4duwHnSZpKFgwNkrRiOnZDLgiZnOq8PXAzMFDSAKAlIh4Hdk+vR4CHgY3JgoLyul0TEYsj4iXgrk620czMukAtDws6mexiv2/qxp2UO7Ygt704t7+4ive4iOyO9jHav8iWv88ioH/+YEQslLQ1sCvZhfD7wBc6KLPcu7ny7lU2IXEnoE9EVJyYmEwmu+ivQ3ZX/hMgyOYiVKPUtkW0/5mpQlqQ9bpsIGlVsjkQv0rHlgG2zV3ss0IkyLUVeIhsmOIp4HZgFeBIsgt26X1PiYjf1Vi3vGrbaGZmXaDWnoAX0vboRlUgIv4GrEU2nHBlPWVJGggMjohbgGPIuq0B3gZWTO83F/hXbvz5m8BfadvlqV4dBSh3A4cA/0zd828AXwburZD3w/p0wt3AwZDNrCfrYn8rIgKYCPwWeDQiXk/5byMLhkjnjKhUaES8DzwHHAA8QBbUjOWjnoxbgW+lzxhJQyWtVlbMPcD+aW7A6mST/jpSz2dhZmZ1qCUIOA04RdK9QJ8G1+Nq4N6I+Fed5awI3CRpGtmF/diUPgE4Lk14Wx84jGyi4DSyQOGX7ZR5BfAJOghQ0vg2ZBdpyC6Ib7bRpkuBC8omBlZrHDAy1f1UsraUXEUWiOSHNH5Yyi9pFtlEvLZMBl5OcxgmA59KP4mI24A/AfdLmg5cy9IX7+uA58m++fA74G9kw0PtuRHYt72JgWZm1hzKbiC7uRLSTWSTw/63u+tSLs1u3zsivtnddekNJA2MiHckrQw8CGyX5gc0RL8hw2LIYWc1qjgzs16h3rUDJE2JiKWez9Ot47LpK20PAn/voQHAucAeZN36Vp2b0u+1L3ByIwMAgM2GDqa1SQtpmJkVTbcGARHxJrBhPi3dQVYKCHbNjXN3iYj4QXmapPP56OtzJWdHREdzBmoi6XDg38uS742I7zXyfRotInbu7jqYmVl1etwM7XShH9Hd9WhLV12EU1DR0MDCzMwsr8cFAWbtmf7CXFqOv7m7q2FmBVbv+HxP4lUEzczMCspBgJmZWUE5CLCqSVpD0oS0KNKstEDQhunYsZLmp0cYl/LvLGluegZA6bVbR2WZmVnX8JwAq4qy5wxPBC6LiFEpbQSwOvAP4CCyRw/vS/YwpJLJEbFnjWWZmVkXcBBg1doF+CAiLigllBZzSk9hHAgcR7YOxKWdLcvMzLqOgwCr1qZ8tJhQuYPIHqs8GdhI0mppeWXIlieemsu7fwdlLUXSGGAMQJ9Bq9ZYbTMza4vnBFgjjAImpIWTrge+njs2OSJG5F5P1lp4RIyPiJERMbLPgMEdn2BmZlVxEGDVmglsWZ4oaXNgGHC7pNlkAcFBnSnLzMy6loMAq9adQD9JR5YSJG0FnA2Mi4iW9FoTGCppnVrLkrRTsypvZmZLcxBgVYlsucl9gS+mr/XNJFvWeGeymf55E8l6BCDNCci9vtZOWXO6oClmZpZ4YqBVLSLmAAdUke9Hud2Kg/jVlmVmZs3jIMB6FS8lbGbWOB4OMDMzKygHAWZmZgXl4QDrVbyUsJV8nJZzNesu7gkwMzMrKAcBZmZmBeUgwOomKSSdkdsfK2lcbv9QSTMkzUzLBo9N6ZdKejo9P+BhSdt2Q/XNzArLQYA1wgJgP0mrlB+QtAdwDLB7RAwHtgDm5rIcFxEjgOOB3zW/qmZmVuIgwBphITAeOLbCsROAsenhQETE/Ii4sEK+u4ENmldFMzMr5yDAGuV84GBJ5U8IrHbZ4K8C0ysdkDRGUquk1kXz5lbKYmZmneAgwBoiIt4CLgd+WOOpv5E0FRgDHNFG2V5K2MysCRwEWCOdRXYhXyGX1tGywcdFxIiI+GJEzGhm5czMbEkOAqxhIuIN4GqWvKM/BThN0hoAkvpJqrW3wMzMmsBBgDXaGcCH3xKIiFvI5gvckZYMnoKfVGlm1iP4j7HVLSIG5rZfBgaUHb8EuKTCeaObXjkzM2uTgwDrVbyUsJlZ43g4wMzMrKAcBJiZmRWUgwAzM7OC8pwA61WmvzCXluNv7u5qWBeZ7fkfZk3lngAzM7OCchBgZmZWUA4CrGaSFkmamnu1pPRjJc0vLSIk6VJJR5Wdu4+kW9L2GpImSHpS0ixJt0jasMsbZGZWUA4CrDPeS8/7L71mp/SDgIeAfdP+lcCosnNHAVdKEjARmBQR60fEJsBPgdWbX30zMwMHAdYgktYHBgInkQUDAHcAG0sakvIMAHYD/gzsAnwQEReUyoiIqRExuSvrbWZWZA4CrDP654YCJqa0g8ju/CcDG0laLSIWAdcDB6Q8ewF3RcTbwKZk6wh0SNIYSa2SWhfNm9vYlpiZFZiDAOuM/HBAqet/FDAhIhaTXfi/ntLzQwKj0n5NImJ8RIyMiJF9Bgyut+5mZpb4OQFWN0mbA8OA27OhfvoCT5GtHngvMETSZ4DP81FAMBP4WtfX1szMStwTYI1wEDAuIlrSa01gqKR1IiKAq4HLgFsiYn46506gn6QjS4VI2krSTl1eezOzgnIQYI0wimymf95EPrrrvxL4DDChdDAFB/sCX0xfEZwJjAPmNL22ZmYGeDjAOiEiBpbtr1shz49y248AqpBnDh9NGjQzsy7mIMB6lc2GDqbVz5M3M2sIDweYmZkVlIMAMzOzgvJwgPUqXkq4sbxUr1mxuSfAzMysoBwEmJmZFZSDAKtJe8v/li8lnNJ2lnRThXKWk3SqpH9KmiHpQUl7dGVbzMyKzkGAVa2K5X/LlxJuz8nAEGDTiNgU+CqwYuNrbWZmbfHEQKtFxeV/YYmlhI8jCwwubauQtKTwkcC6EbEglfMy2eOFzcysi7gnwGrR3vK/Sy0l3E45GwDPRsRb1byplxI2M2sOBwHWKG0tJVw3LyVsZtYcHg6wWlRc/reDpYQreQJYW9KKEfF2k+pqZmYdcE+A1aLi8r/A2bSxlHClQiJiHvB74BxJfVM5QyQd0vwmmJlZiYMAq1o7y//uTPtLCe8q6fnca1vgJOBVYJakGcCf076ZmXURDwdYTapd/je/lDDQv41sP04vMzPrBg4CrFfxUsJmZo3j4QAzM7OCchBgZmZWUB4OsF7FSwl7+V8zaxz3BJiZmRWUgwAzM7OCchBQMJJC0h9y+8tKelXSTZIOlzQ1vd6XND1tnyppdMo3VdJjko4tK/ezqex/q/B+Z+T2x0oal7Y3kjQplfmopPFNbr6ZmeU4CCied4FNJZW+u/9F4AWAiLgkIkZExAhgDrBL2j8+5b0qHdsOOFHSWrlyDwLuST/zFgD7SVqlQl3OAc5M7/Fp4Nz6m2dmZtVyEFBMfwFKs8tKq/9VLSJeJ3v+/xAAZQsGfA0YDewuaflc9oXAeOBYljYEeD5X7vRa6mFmZvVxEFBME4BR6WK9OfC3Wk6WtDawPDAtJW0HPB0RTwKTgC+XnXI+cLCk8iUAzwTulPQXScdKWqmmVpiZWV0cBBRQREwDWsh6AW6p4dQD03oBTwFnR8T8lH4QWWBB+rnEkEBEvAVcDvywLP0S4NPANWTrDzwgqV/5m0oaI6lVUuuieXNrqK6ZmbXHQUBx3QCcTm1DAVdFxHBgB+AMSWtI6gPsD/xM0myycf09JK1Ydu5ZwBHACvnEiJgTERdHxN5kQweblr9pRIyPiJERMbLPgPLOBDMz6ywHAcV1MfDLzozDR8T9wB+Afwd2A/4eEWulZYTXAa4D9ik75w3garJAAABJX5K0XNpeA1iZNEnRzMyaz0FAQUXE8xFxdh1F/BdwODCGpZcRvg74RoVzzgDy3xLYHZgh6e/ArcBxEfFSHXUyM7MaKFsi3qx36DdkWAw57Kzurka38mODzaxWkqZExMjydPcEmJmZFZQXELJeZbOhg2n1nbCZWUO4J8DMzKygHASYmZkVlIcDrFeZ/sJcWo6/ubur0SU8AdDMms09AWZmZgXlIMDMzKygHARYVdIjgidIelLSLEm3SNpQ0oyyfOMkjU3bv5H0mKRpkibmFwiStL2kB9PxxySN6eImmZkVnoMA61BaKngiMCki1o+ITYCfAqt3cOrtwKYRsTnwD+CEVN4awJ+AoyNiY2B74ChJHgQ3M+tCDgKsGrsAH0TEBaWEiJgKPNfeSRFxW0QsTLsPAJ9K298DLo2Ih1O+14AfA8c3uN5mZtYOfzvAqrEpMKWNY+tLmprbX4NsdcJy3wKuStvDgcvKjrem9KWkoYIxAH0GrVpdjc3MrEMOAqxeT0bEiNKOpHHlGSSdSLZM8BWlJKDSohUVF7KIiPHAeMjWDqivumZmVuLhAKvGTGDLzpwo6TBgT+Dg+Gi1qplA+UIWWwKzOl1DMzOrmYMAq8adQD9JR5YSJG0FrNPeSZK+BPwE2Csi5uUOnQ+MljQi5VuZbGni0xpcbzMza4eDAOtQuoPfF/hi+orgTGAcMKeDU88DVgRulzRV0gWpvBeBQ4ALJT0G3AdcHBE3NqsNZma2NM8JsKpExBzggAqHNi3LNy63vUE75d0NbNWo+pmZWe0cBFiv4qWEzcwax8MBZmZmBeUgwMzMrKA8HGC9ysdxKWEvGWxm3cU9AWZmZgXlIMDMzKygHARYQ0g6UdLMtGzwVEnbSFpO0qmS/ilpRlo6eI+Uf6Ck35WeOyDpbknbdHc7zMyKxHMCrG6StiV7NPAWEbFA0ipAX+BkYAjZcsILJK0O7JROuwh4GhgWEYslrQd8uhuqb2ZWWA4CrBGGAK9FxALIlgaWNAA4Elg3l/4ycLWk9YFtyNYTWJyOPQU81S21NzMrKA8HWCPcBqwl6R+S/lvSTsAGwLMR8VaF/MOBqRGxqEtraWZmS3AQYHWLiHfIVgEcA7wKXAXs3KjyJY2R1CqpddG8uY0q1sys8DwcYA2R7uonAZMkTQeOAtaWtGJEvF2WfSbwGUnLlIYDOih7PDAeoN+QYdFBdjMzq5J7AqxukjaSNCyXNAJ4HPg9cI6kvinfEEmHRMSTQCvwC0lKx4ZJ2ruLq25mVmjuCbBGGAicK2klYCHwBNnQwFvAr4BZkuYD7wI/S+d8GzgDeELSPOB14LgurreZWaE5CLC6RcQU4PNtHP5xepWf8xbZtwfMzKybeDjAzMysoNwTYL3KZkMH0+oFd8zMGsI9AWZmZgXlIMDMzKygPBxgvcr0F+bScvzN3fLesz0MYWYfM+4JMDMzKygHAWZmZgXlIMCQtLqkP0l6StIUSfdL2rcB5e4s6aZG1NHMzBrPQUDBpcf2/hm4OyLWi4gtgVHAp7qhLp6jYmbWhRwE2BeA9yPiglJCRDwTEedK6iPpN5IekjRN0lHw4R3+JEnXSnpM0hW5NQC+lNLuAfYrlSlpBUkXp7IeKa0TIGm0pGsk3Ui2JLGZmXUR33nZcODhNo4dAcyNiK0k9QPulVS6UH82nTsHuBfYTlIrcCFZYPEE2ZLCJScCd0bEt9IaAw9KuiMd2xbYPCLeqFQJSWPI1iKgz6BVO9dKMzNbioMAW4Kk84HtgfeBZ4DNJX0tHR4MDEvHHoyI59M5U4EW4B3g6Yj4Z0r/I+niDewO7CVpbNpfHlg7bd/eVgAAXkrYzKxZHATYTGD/0k5EfE/SKmRL/T4L/CAibs2fIGlnYEEuaREf/Vtq6yItYP+IeLysrG3IVhc0M7Mu5jkBdiewvKTv5NIGpJ+3At+RtByApA0lrdBOWY8B60paP+0flDt2K/CD3NyBzzak9mZm1mkOAgouIgLYB9hJ0tOSHgQuA34CXATMAh6WNAP4He30HkXEfLLu/5vTxMBncodPBpYDpqWyTm5Cc8zMrAbKrgFmvUO/IcNiyGFndct7+7HBZtZbSZoSESPL0z0nwHoVLyVsZtY4Hg4wMzMrKAcBZmZmBeXhAOtVunIpYc8BMLOPO/cEmJmZFZSDADMzs4JyEGAfknSipJlpsaCpkrZJCwU9W3rIT8r3Z0nv5PaHS7pT0j8k/VPS/8k9FGi0pPPS9jKSLksLCUnSbEnT03tNlXRO17fazKy4PCfAAJC0LbAnsEVELEiPDu6bDr8JbAfckxb/GZI7rz9wA/CdiLhN0gDgOuC7wPm5fAIuIHtg0OERESlO2CUiXmty88zMrAL3BFjJEOC1iFgAEBGvRcScdGwCMCpt7wdcnzvvG8C9EXFbOm8e8H3g+LLyzwZWBg6NiMXNaYKZmdXCQYCV3Aaslbr0/1vSTrlj/wvsKKkPWTCQXyJ4ODAlX1BEPAkMlDQoJX0D2BIYFRELy973rtxwwLGVKiZpjKRWSa2L5s3tfAvNzGwJHg4wACLiHUlbAjsAuwBXSSrdzS8C7gEOBPpHxOz8FAHaXjmwlP4wsDGwNXBvWZ4OhwO8lLCZWXO4J8A+FBGLImJSRPycrEt//9zhCcC5wNVlp80ElngetaT1gHci4u2U9BhwAFlgMbwplTczs5o5CDAAJG0kaVguaQRLrgI4GTgFuLLs1CuA7SXtlsrpD5wDnJbPFBH3AUeTrTC4dmNrb2ZmneHhACsZCJybZv8vBJ4gWxb4WvhwyeHTy0+KiPck7Z3OPR/oA/wBOK9C3pskrQr8P0k7pOS7JC1K29Mi4tDGNsvMzNripYStV+nKpYT92GAz+7hoaylhDweYmZkVlIcDrFfZbOhgWn2HbmbWEO4JMDMzKygHAWZmZgXl4QDrVaa/MJeW429u+vt4UqCZFYF7AszMzArKQYCZmVlBeTjA6iZpZbJFhgDWIFtr4FWgBZgTEZvk8o4je6Tw6ZIuBXYCSqsCzYuIz3dRtc3MCs9BgNUtIl4ne8xw+UW+Bbipg9OPi4hrm1pBMzOryMMBZmZmBeUgwLrbbyRNTa8rKmWQNEZSq6TWRfPmVspiZmad4OEAa6a2FqbIp3c4HBAR44HxkK0d0KC6mZkVnnsCrJleBz5RlvZJ4LVuqIuZmZVxEGBNExHvAC9K2hVA0ieBLwH3dGvFzMwMcBBgzXcocJKkqcCdwC8i4snc8fycgKmS+nZLLc3MCshzAqyhImJc2f4sYJc28o7ugiqZmVkbHARYr+KlhM3MGsfDAWZmZgXlIMDMzKygPBxgvUozlxL28sFmVjTuCTAzMysoBwFmZmYF5SDAGkrSorLv/bek9GMlzZc0OJd3Z0lzc3nv6LaKm5kVkOcEWKO9FxEjKqQfBDwE7AtcmkufHBF7dkG9zMysjHsCrOkkrQ8MBE4iCwbMzKwHcBBgjdY/170/MaUdBFwJTAY2krRaLv8OufwnVirQSwmbmTWHhwOs0SoNB4wC9o2IxZKuB74OnJ+OdTgc4KWEzcyaw0GANZWkzYFhwO2SAPoCT/FREGBmZt3EwwHWbAcB4yKiJb3WBIZKWqe7K2ZmVnQOAqzZRgETy9ImpnQzM+tGHg6whoqIgWX761bI86Pc7qRm18nMzCpzEGC9ipcSNjNrHA8HmJmZFZSDADMzs4JyEGBmZlZQnhNgvcr0F+bScvzNDS93tucZmFkBuSfAzMysoBwEmJmZFZSDAKuapEW5xX6mSmpJ6VtLulvS45Iek3SRpE0kPS9pmbIypkraOm0fKmmGpJmSZkka2w3NMjMrLM8JsFostTiQpNWBa4BREXG/sgUC9gdeB54DdgD+mvJuDKwYEQ9K2gM4Btg9IuZIWh74Zpe1xMzM3BNgdfsecFlE3A8QmWsj4mWy5YPzjwceldIATgDGRsScdN78iLiwC+ttZlZ4DgKsFv1zQwGl9QA2Baa0kf9qYB9JpR6nA4EJVZy3BEljJLVKal00b25n625mZmU8HGC1WGo4oD0R8ZKkmcCukl4GPoiIGbW+aUSMB8YD9BsyLGo938zMKnNPgNVrJrBlO8dLQwL5oYBqzjMzsyZzEGD1Og84TNI2pQRJh0haI+1eB3yZJYcCAE4BTivlk9RP0g+7qM5mZoaHA6xOEfGypFHA6ZJWAxYDdwPXp+NvSnoAWD0ins6dd0v6ZsEd6RsFAVzc9S0wMysuBwFWtYgY2Eb6/WRfBWzrvL3bSL8EuKQxtTMzs1o5CLBeZbOhg2n1c/7NzBrCcwLMzMwKykGAmZlZQTkIMDMzKygHAWZmZgXlIMDMzKygHASYmZkVlIMAMzOzgnIQYGZmVlAOAszMzApKEV6Z1XoPSW8Dj3d3PZpsFeC17q5EkxWhjVCMdrqNvcM6EbFqeaIfG2y9zeMRMbK7K9FMklrdxo+HIrTTbezdPBxgZmZWUA4CzMzMCspBgPU247u7Al3Abfz4KEI73cZezBMDzczMCso9AWZmZgXlIMDMzKygHARYjyDpS5Iel/SEpOMrHJekc9LxaZK2qPbcnqKzbZS0lqS7JD0qaaakf+/62levnt9lOt5H0iOSbuq6Wtemzn+vK0m6VtJj6Xe6bdfWvjp1tvHY9G91hqQrJS3ftbWvXhXt3FjS/ZIWSBpby7m9QkT45Ve3voA+wJPAekBf4O/AJmV5vgz8BRDwOeBv1Z7bE151tnEIsEXaXhH4R09sY73tzB3/EfAn4Kbubk8z2ghcBnw7bfcFVuruNjX43+tQ4Gmgf9q/Ghjd3W2qo52rAVsB/wmMreXc3vByT4D1BFsDT0TEUxHxPjAB2Lssz97A5ZF5AFhJ0pAqz+0JOt3GiHgxIh4GiIi3gUfJ/tD2RPX8LpH0KeArwEVdWekadbqNkgYBOwK/B4iI9yPizS6se7Xq+j2SPYiuv6RlgQHAnK6qeI06bGdEvBIRDwEf1Hpub+AgwHqCocBzuf3nWfoi11aeas7tCepp44cktQCfBf7W+Co2RL3tPAv4MbC4SfVrhHrauB7wKnBJGvK4SNIKzaxsJ3W6jRHxAnA68CzwIjA3Im5rYl3rUc/fj97yt6ddDgKsJ1CFtPLvrraVp5pze4J62pgdlAYC1wHHRMRbDaxbI3W6nZL2BF6JiCmNr1ZD1fO7XBbYAvifiPgs8C7QE8eS6/k9foLsjnhdYE1gBUmHNLh+jVLP34/e8renXQ4CrCd4Hlgrt/8plu4+bCtPNef2BPW0EUnLkQUAV0TE9U2sZ73qaed2wF6SZpN1rX5B0h+bV9VOq/ff6/MRUerJuZYsKOhp6mnjbsDTEfFqRHwAXA98vol1rUc9fz96y9+edjkIsJ7gIWCYpHUl9QVGATeU5bkBODTNSP4cWRfji1We2xN0uo2SRDaG/GhE/LZrq12zTrczIk6IiE9FREs6786I6Il3kPW08SXgOUkbpXy7ArO6rObVq+f/5LPA5yQNSP92dyWbx9IT1fP3o7f87WmXVxG0bhcRCyV9H7iVbMbtxRExU9LR6fgFwC1ks5GfAOYBh7d3bjc0o131tJHsDvmbwHRJU1PaTyPili5sQlXqbGev0IA2/gC4Il04nqIHtr/O/5N/k3Qt8DCwEHiEHvrY3WraKWkNoBUYBCyWdAzZtwDe6g1/ezrixwabmZkVlIcDzMzMCspBgJmZWUE5CDAzMysoBwFmZmYF5SDAzMysoBwEmJmZFZSDADMzs4L6/xL8kdMCVYL7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEICAYAAACXo2mmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAdJElEQVR4nO3de7xd853/8de7B4mIHEXGRH44LikVITioa93a39BxK5oO1VDTVP1Mhw6a0l8nzG9a4zIuvxoaRoUqrdv8XKaouFPihCQnqWAqMcQ1DRFC1Mnn98f6HraTc8vJd+999tnv5+OxH2fttb7f7/qulZ3zPt/13XsvRQRmZmY5fKbaHTAzs4HDoWJmZtk4VMzMLBuHipmZZeNQMTOzbBwqZmaWjUPF6pKkJkkhabUMbW0g6SFJSyRdkKN/OUk6Q9KV1e6H1QeHipWdpPmSPpS0fof1M9Iv9qZethOStihLJ1fc13xJ+/ey+ARgITAsIv6hjN3qkaS9Jb1cui4ifhIRf1uGfa0h6QJJL0t6V9I8SRfm3o/VFoeKVco84G/an0gaA6xZve5ktQnwh+jDJ4lzjJSq6IdAM7AzsDawD/B0zh3U+PmpSw4Vq5RrgW+WPB8PXFNaQNIDkv625Pmxkh5Jyw+l1TPTX8XjSreX1Pl4NCPpK5KelvSOpJckTepLx9v3I+l8SW+lv8gPSNuuTsdyeurX/pIGSbpI0ivpcZGkQan83ukv+x9Ieg34haRJkm6U9Mt0Ca1V0uck/VDSG6nvXy7pz3GSnkllX5D0nbR+LeC3wIapL+9K2jC1/8uS+gdLmiPp7XTOP1+ybb6kUyXNkrRY0q8lDe7i1OwE3BoRr0RhfkRcU9LWRpJukfSmpD9J+lla/xlJP5L0Yjq+ayQ1pm3tlyWPl/TfwH1p/bfSMb8l6W5Jm6T1knRhamdx6vc2ffl3tjwcKlYpjwPDJH1eUgMwDvhlD3U+FhF7pcXtImJoRPy6F9XeowiydYCvAN+VdOhK9foTuwDPAusD5wL/LkkRcSxwHXBu6te9wJnAF4CxwHYUf8n/qKStvwTWpRjhTEjrDqII3s9S/LV/N8X/z5HA2cDPS+q/Afw1MAw4DrhQ0g4R8R5wAPBK6svQiHil9CAkfQ64HjgZGA78J3C7pDVKin0N+CtgU2Bb4NguzsnjwPclnShpjCSV7KcBuAN4EWhKx3FD2nxseuwDbAYMBX7Woe0vAp8H/mf6NzsD+Grq88PpGAC+DOwFfI7i33kc8Kcu+msV4FCxSmofrXwJmAssKOfOIuKBiGiNiOURMYviF9EX+9jcixFxRUS0AVOAEcAGXZQ9Gjg7It6IiDeBs4BjSrYvB/4xIpZFxPtp3cMRcXdEfATcSPHL85yI+DPFL+MmSeuk47ozIv6YRgcPAvcAe/byOMYBd0bE71Lb51NchtytpMwlafSxCLidIhw781PgX9LxtgALJI1P23YGNgROi4j3IuKDiGgfVR4N/GtEvBAR71JcRvt6h0tdk1K994HvAD+NiGfS+fkJMDaNVv5MceltK0CpzKu9PBdWBg4Vq6RrgaMo/kq9pvuiq07SLpLuT5dfFgMnUIw0+uK19oWIWJoWh3ZRdkOKv9DbvZjWtXszIj7oUOf1kuX3gYUpwNqff7w/SQdIelzSIklvAwfS++P6VN8iYjnwEsVIot1rJctL6eI4I6ItIi6NiN0pRgn/DFyVLqdtRBHEH/XUh7S8Gp8O6ZdKljcBLk6X694GFgECRkbEfRSjnEuB1yVNljSsi2O3CnCoWMVExIsUE/YHArd0UuQ9YEjJ87/soclPlZfUsfyvgNuAjSKiEbic4pdRub1C8Yuw3cZpXbs+fzV4mpu5mWKEsUFErENxCav9uHpq+1N9S5esNmIVR40R8X5EXAq8BWxNEQobq/OJ9s7Oz0d8OlhLj+Ml4DsRsU7JY82IeCzt+5KI2BEYTXEZ7LRVORZbNQ4Vq7TjgX3T9f+OZgBflTQkTbYf32H76xTX4NvNBEZLGpsmkyd1KL82sCgiPpC0M8UoqRKuB34kabiKt1H/mJWYP+rBGsAg4E3go/SGgS+XbH8dWK994rsTvwG+Imk/SasD/wAsAx5b2Y5IOjm98WBNSaulS19rU8wJTQNeBc6RtJakwZJ2T1WvB06RtKmkoRSXs37dxagGij8GfihpdNpvo6Qj0/JOaUS6OsUfGR8AbV20YxXgULGKSnMBLV1svhD4kOIX4xSKCfBSk4Ap6TLI1yLiOYpJ7HuB54FHOpQ/EThb0hKKX+y/yXMUPfo/FHMMs4BW4Km0bpVFxBLgexTH8hZFUN5Wsn0uxS/tF9J52rBD/WeBbwD/l+KzNQcBB0XEh33ozvvABRSXyxYC/ws4PM2VtKW2twD+G3iZYj4H4CqKS6EPUYxcPwD+rptjvpVi7uYGSe8AsynekADFmxWuSOfiRYpJ+vP7cCyWiXyTLjMzy8UjFTMzy8ahYmZm2ThUzMwsG4eKmZllU/df1rb++utHU1NTtbthZlZTpk+fvjAihndcX/eh0tTUREtLV+9wNTOzzkh6sbP1vvxlZmbZOFTMzCwbh4qZmWXjUDEzs2wcKmZmlo1DxczMsnGomJlZNg4VMzPLpu4//Ni6YDFNE++sdjfMzCpq/jlfKUu7HqmYmVk2DhUzM8vGoWJmZtk4VMzMLBuHipmZZVOxUJG0t6TdKrU/MzOrvEqOVPYGHCpmZgNYj6EiqUnSXElTJM2SdJOkIZL2k/S0pFZJV0kalMrPl7R+Wm6W9ICkJuAE4BRJMyTtKWkDSbdKmpkeu6U635c0Oz1O7tCHK9P66yTtL+lRSc9L2jmVWyv15cnUt0PKc9rMzKwzvR2pbAlMjohtgXeA7wNXA+MiYgzFhyi/21XliJgPXA5cGBFjI+Jh4BLgwYjYDtgBmCNpR+A4YBfgC8C3JW2fmtkCuBjYFtgKOArYAzgVOCOVORO4LyJ2AvYBzpO0Vsf+SJogqUVSS9vSxb08BWZm1pPehspLEfFoWv4lsB8wLyKeS+umAHut5L73BS4DiIi2iFhMERK3RsR7EfEucAuwZyo/LyJaI2I5MAeYGhEBtAJNqcyXgYmSZgAPAIOBjTvuOCImR0RzRDQ3DGlcyW6bmVlXevs1LbESbX7EJ2E1eOW6g7rZtqxkeXnJ8+V8chwCDo+IZ1dyv2ZmlkFvRyobS9o1Lf8NcC/QJGmLtO4Y4MG0PB/YMS0fXtLGEmDtkudTSZfMJDVIGgY8BBya5mzWAg4DHu794XA38HeSlNrdvofyZmaWUW9D5RlgvKRZwLrAhRRzHzdKaqUYLVyeyp4FXCzpYaCtpI3bgcPaJ+qBvwf2SfWnA6Mj4imKuZppwBPAlRHx9Eoczz8BqwOzJM1Oz83MrEJUTEt0U6B459YdEbFNRXpUYYNGjIoR4y+qdjfMzCpqVb+lWNL0iGjuuN6fqDczs2x6nKhPbwcekKMUMzPLyyMVMzPLpu7v/DhmZCMtZboDmplZvfFIxczMsnGomJlZNg4VMzPLxqFiZmbZ1P1EfeuCxTRNvLPa3TAz67NV/SBjTh6pmJlZNg4VMzPLxqFiZmbZOFTMzCwbh4qZmWVT06EiaR1JJ5Y831vSHdXsk5lZPavpUAHWAU7sqZCZmVVGxUJFUpOkuZKulDRb0nWS9pf0qKTnJe0saV1J/yFplqTHJW2b6k6SdJWkByS9IOl7qdlzgM3T3STPS+uGSrop7eu69lsLm5lZ+VX6w49bAEcCE4AngaOAPYCDgTOAl4CnI+JQSfsC1wBjU92tgH0o7nP/rKTLgInANhExForLX8D2wGjgFeBRYHfgkdJOSJqQ+kDDsOHlOE4zs7pU6ctf8yKiNSKWA3OAqVHcz7gVaKIImGsBIuI+YD1JjanunRGxLCIWAm8AG3Sxj2kR8XLax4zU7qdExOSIaI6I5oYhjR03m5lZH1U6VJaVLC8veb6cYtTU2aWq6KRuG12PsnpbzszMMutvE/UPAUfDx5eyFkbEO92UX0JxOczMzPqB/vZX/CTgF5JmAUuB8d0Vjog/pYn+2cBvAX8zpJlZFamY0qhfg0aMihHjL6p2N8zM+qwa31IsaXpENHdc398uf5mZWQ1zqJiZWTYOFTMzy6a/TdRX3JiRjbT0o7ummZnVMo9UzMwsG4eKmZll41AxM7Ns6n5OpXXBYpom+jOTZh1V47MPVvs8UjEzs2wcKmZmlo1DxczMsnGomJlZNg4VMzPLpl+GSrqf/VGrUP9YSRvm7JOZmfWsX4YKxS2A+xwqwLGAQ8XMrMLKEiqSvilplqSZkq6VtImkqWndVEkbp3JXS7pE0mOSXpB0RGriHGBPSTMknZJGLg9Leio9divZ1+mSWtO+zkltNAPXpfprluMYzcxsRdk//ChpNHAmsHtELJS0LjAFuCYipkj6FnAJcGiqMgLYA9gKuA24CZgInBoRf53aHAJ8KSI+kDQKuB5olnRAameXiFgqad2IWCTppFS/pYs+TgAmADQMG577FJiZ1a1yjFT2BW6KiIUAEbEI2BX4Vdp+LUWItPuPiFgeEX8ANuiizdWBKyS1AjcCW6f1+wO/iIilJfvqUURMjojmiGhuGNK4EodmZmbdKcfXtAjo6R7FpduXdajbmVOA14HtKILwg5XYl5mZVUg5RipTga9JWg8gXf56DPh62n408EgPbSwB1i553gi8GhHLgWOAhrT+HuBb6fJY+746q29mZhWQfaQSEXMk/TPwoKQ24Gnge8BVkk4D3gSO66GZWcBHkmYCVwP/Btws6UjgfuC9tK+7JI0FWiR9CPwncEaqc7mk94FdI+L9vEdpZmadUUR9Xz0aNGJUjBh/UbW7Ydbv+FuKrTuSpkdEc8f1/fVzKmZmVoMcKmZmlo1DxczMsqn7Oz+OGdlIi68dm5ll4ZGKmZll41AxM7NsHCpmZpaNQ8XMzLKp+4n61gWLaZp4Z7W7Yf2UPwBotnI8UjEzs2wcKmZmlo1DxczMsnGomJlZNv0yVCS9W+0+mJnZyuuXoWJmZrWpX4eKCudJmi2pVdK4tH6opKmSnkrrD0nrmyQ9I+kKSXMk3SNpzeoehZlZ/ejXoQJ8FRhLcW/6/YHzJI2guEf9YRGxA7APcIGk9vvbjwIujYjRwNvA4ZXutJlZvervobIHcH1EtEXE68CDwE6AgJ9ImgXcC4wENkh15kXEjLQ8HWjq2KikCZJaJLW0LV1c5kMwM6sf/T1U1MX6o4HhwI4RMRZ4HRicti0rKddGJ98aEBGTI6I5IpobhjRm7K6ZWX3r76HyEDBOUoOk4cBewDSgEXgjIv4saR9gk2p20szMCv39u79uBXYFZgIBnB4Rr0m6DrhdUgswA5hbvS6amVm7fhkqETE0/QzgtPQo3b6QImw6s01JufPL1UczM1tRf7/8ZWZmNcShYmZm2ThUzMwsG4eKmZll0y8n6itpzMhGWnx3PzOzLDxSMTOzbBwqZmaWjUPFzMyyqfs5ldYFi2maeGe1u2FVNt/zamZZeKRiZmbZOFTMzCwbh4qZmWXjUDEzs2wcKmZmlk22UJHUJGl2rvY6af+xcrVtZmZ51MxIJSJ2q3YfzMyse7lDpUHSFZLmSLpH0pqSxkp6XNIsSbdK+iyApAckNafl9SXNT8ujJU2TNCPVGZXWv5t+7p3q3iRprqTrJCltOzCte0TSJZLuyHx8ZmbWjdyhMgq4NCJGA28DhwPXAD+IiG2BVuAfe2jjBODiiBgLNAMvd1Jme+BkYGtgM2B3SYOBnwMHRMQewPCudiBpgqQWSS1tSxf3/ujMzKxbuUNlXkTMSMvTgc2BdSLiwbRuCrBXD238HjhD0g+ATSLi/U7KTIuIlyNiOcU96puArYAXImJeKnN9VzuIiMkR0RwRzQ1DGntxWGZm1hu5Q2VZyXIbsE43ZT8q2f/g9pUR8SvgYOB94G5J+/ZiP6sB6kN/zcwso3JP1C8G3pK0Z3p+DNA+apkP7JiWj2ivIGkzihHHJcBtwLa93NdcYDNJTen5uL5328zM+qISXyg5Hrhc0hDgBeC4tP584DeSjgHuKyk/DviGpD8DrwFn92YnEfG+pBOBuyQtBKblOgAzM+sdRUS1+5CNpKER8W56N9ilwPMRcWF3dQaNGBUjxl9Ukf5Z/+VvKTZbOZKmR0Rzx/U18zmVXvq2pBnAHKCR4t1gZmZWIQPqfippVNLtyMTMzMpnoI1UzMysigbUSKUvxoxspMXX083MsvBIxczMsnGomJlZNg4VMzPLxqFiZmbZ1P1EfeuCxTRNvLPa3agL/oCh2cDnkYqZmWXjUDEzs2wcKmZmlo1DxczMsqm5UJH0WDfb9vZ96c3MqqfmQiUidqt2H8zMrHM1FyqS3lXhPEmzJbVKKr3L4zBJt0r6g6TLJdXcMZqZ1apa/ZzKV4GxwHbA+sCTkh5K23YGtgZeBO5KZW+qQh/NzOpOrf4VvwdwfUS0RcTrFPe93yltmxYRL0REG3B9KvspkiZIapHU0rZ0ceV6bWY2wNVqqKibbR3vj7zC/ZIjYnJENEdEc8OQxrw9MzOrY7UaKg8B4yQ1SBoO7AVMS9t2lrRpmksZBzxSrU6amdWbWgyVAG4FZgEzgfuA0yPitbT998A5wGxgXiprZmYVUFMT9ZLWAxZFRACnpcfHIuIB4IHK98zMzKCGRiqSNqQYhZxf7b6YmVnnamakEhGvAJ+rdj/MzKxrNTNSMTOz/s+hYmZm2dTM5a9yGTOykRbfkdDMLAuPVMzMLBuHipmZZeNQMTOzbOp+TqV1wWKaJt7Z5/rzPR9jZvYxj1TMzCwbh4qZmWXjUDEzs2wcKmZmlo1DxczMsqlYqEiaJOnUbrYPl/SEpKcl7dmH9o+V9LO0fKikrVelv2ZmtvL600hlP2BuRGwfEQ+vYluHAg4VM7MKK2uoSDpT0rOS7gW2TOs2l3SXpOmSHpa0laSxwLnAgZJmSFpT0mWSWiTNkXRWSZvzJa2flpslPdBhn7sBBwPnpbY2L+cxmpnZJ8r24UdJOwJfB7ZP+3kKmA5MBk6IiOcl7QL8W0TsK+nHQHNEnJTqnxkRiyQ1AFMlbRsRs3rab0Q8Juk24I6IuKmLvk0AJgA0DBu+6gdrZmZAeT9Rvydwa0QsBUi/6AcDuwE3SmovN6iL+l9Lv/xXA0ZQXM7qMVR6IyImU4Qbg0aMihxtmplZ+b+mpeMv7M8Ab0fE2O4qSdoUOBXYKSLeknQ1RSABfMQnl+0Gd1LdzMyqpJxzKg8Bh6X5kbWBg4ClwDxJRwKosF0ndYcB7wGLJW0AHFCybT6wY1o+vIt9LwHWXvVDMDOzlVG2UImIp4BfAzOAm4H2d3QdDRwvaSYwBzikk7ozgafT9quAR0s2nwVcLOlhoK2L3d8AnJbenuyJejOzClFEfU8pDBoxKkaMv6jP9f0txWZWjyRNj4jmjuv70+dUzMysxjlUzMwsG4eKmZllU/d3fhwzspEWz4uYmWXhkYqZmWXjUDEzs2wcKmZmlk3dh0rrgsXV7oKZ2YBR96FiZmb5OFTMzCwbh4qZmWXjUDEzs2wcKmZmlk2WUJF0rKSf5WirpM1DJW1d8vxsSfvn3IeZmeXVn0cqh1LcQhiAiPhxRNxbve6YmVlPehUqkr4haZqkGZJ+LqlB0nGSnpP0ILB7SdmrJR1R8vzdkuXTJbVKminpnLTu25KeTOtuljRE0m7AwcB5aZ+bl7Yrab90A65WSVdJGpTWz5d0lqSn0ratspwlMzPrlR5DRdLngXHA7une8m3ANyjuwLg78CVKRhTdtHMAxehjl4jYDjg3bbolInZK654Bjo+Ix4DbgNMiYmxE/LGkncHA1cC4iBhD8aWY3y3Z1cKI2AG4jOI+9531ZYKkFkktbUv94Uczs1x6M1LZj+Ke8E9KmpGenwI8EBFvRsSHFLcN7sn+wC8iYilARCxK67eR9LCkVopbDY/uoZ0tgXkR8Vx6PgXYq2T7LenndKCpswYiYnJENEdEc8OQxl503czMeqM3oSJgShoxjI2ILYFJQFf3If6ovV1JAtYoaaezOlcDJ6VRx1nA4F70pzvL0s82/NX+ZmYV1ZtQmQocIekvACStCzwN7C1pPUmrA0eWlJ9PMbIBOARYPS3fA3xL0pCSdgDWBl5N7Rxd0s6StK2juUCTpC3S82OAB3txHGZmVmY9hkpE/AH4EXCPpFnA74ARFKOV3wP3Ak+VVLkC+KKkacAuwHupnbso5kla0mW09vmO/w08kdqdW9LODcBpaUJ+85L+fAAcB9yYLpktBy5fqaM2M7OyUERXV7Hqw6ARo2LZq89XuxtmZjVF0vSIaO64vj9/TsXMzGqMQ8XMzLJxqJiZWTZ1HypjRvpzKmZmudR9qJiZWT4OFTMzy8ahYmZm2ThUzMwsG4eKmZll41AxM7NsHCpmZpaNQ8XMzLJxqJiZWTYOFTMzy6YmQkXShpJuSstjJR3Yizp7S7qj/L0zM7N2/T5UJK0WEa9ExBFp1Vigx1AxM7PKK1uoSGqSNFfSlZJmS7pO0v6SHpX0vKSd0+OxdHfHxyRtmeoeK+lGSbdT3HGyKbWxBnA2ME7SDEnjumrDzMwqb7Uyt78Fxf3rJwBPAkcBewAHA2cA3wT2ioiPJO0P/AQ4PNXdFdg2IhZJagKIiA8l/RhojoiTACQN66aNTkmakPrExhtvnPFwzczqW7lDZV5EtAJImgNMjYhI95ZvAhqBKZJGAQGsXlL3dxGxqBf76K6NTkXEZGAyQHNzc33fT9nMLKNyz6ksK1leXvJ8OUWg/RNwf0RsAxwEDC4p/14v99FdG2ZmVkHVnqhvBBak5WN7WWcJsPYqtmFmZmVQ7VA5F/ippEeBhl7WuR/Yun2ivo9tmJlZGSiivqcUmpubo6WlpdrdMDOrKZKmR0Rzx/XVHqmYmdkA4lAxM7NsHCpmZpaNQ8XMzLJxqJiZWTYOFTMzy8ahYmZm2ThUzMwsG4eKmZll41AxM7NsHCpmZpaNQ8XMzLJxqJiZWTZ1/y3FkpYAz1a7H/3Q+sDCaneiH/J56ZzPy4oG+jnZJCKGd1xZ7tsJ14JnO/v65nonqcXnZUU+L53zeVlRvZ4TX/4yM7NsHCpmZpaNQwUmV7sD/ZTPS+d8Xjrn87KiujwndT9Rb2Zm+XikYmZm2ThUzMwsmwEVKpL+StKzkv5L0sROtkvSJWn7LEk79FRX0rqSfifp+fTzs5U6nlzKdF4mSVogaUZ6HFip48llFc/LVZLekDS7Q516f710dV7q9vUiaSNJ90t6RtIcSX9fUqfmXy8riIgB8QAagD8CmwFrADOBrTuUORD4LSDgC8ATPdUFzgUmpuWJwL9U+1j7yXmZBJxa7eOrxnlJ2/YCdgBmd6hTt6+XHs5L3b5egBHADml5beC5gfL7pbPHQBqp7Az8V0S8EBEfAjcAh3QocwhwTRQeB9aRNKKHuocAU9LyFODQMh9HbuU6L7VuVc4LEfEQsKiTduv59dLdeal1fT4vEfFqRDwFEBFLgGeAkSV1avn1soKBFCojgZdKnr/MJ/9wPZXpru4GEfEqQPr5Fxn7XAnlOi8AJ6Vh/lU1OGxflfPSnXp+vfSk7l8vkpqA7YEn0qpaf72sYCCFijpZ1/H90l2V6U3dWlWu83IZsDkwFngVuKCP/auWVTkvA1m5zkvdv14kDQVuBk6OiHcy9q1fGUih8jKwUcnz/wG80ssy3dV9vX1on36+kbHPlVCW8xIRr0dEW0QsB66guDxQS1blvHSnnl8vXar314uk1SkC5bqIuKWkTK2/XlYwkELlSWCUpE0lrQF8HbitQ5nbgG+md2l8AVichpzd1b0NGJ+WxwP/r9wHkllZzkv7f4TkMGA2tWVVzkt36vn10qV6fr1IEvDvwDMR8a+d1Knl18uKqv1OgZwPindfPEfxLo0z07oTgBPSsoBL0/ZWoLm7umn9esBU4Pn0c91qH2c/OS/XprKzKP5jjKj2cVb4vFxPcRnnzxR/oR7v10u356VuXy/AHhSXwWYBM9LjwIHyeun48Ne0mJlZNgPp8peZmVWZQ8XMzLJxqJiZWTYOFTMzy8ahYmZm2ThUzMwsG4eKmZll8/8BO0KTsFVBgfAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"MUTUAL INFORMATION FOR OBESITY ==================================\")\n",
    "plot_mutual_information(obesity_data)\n",
    "\n",
    "print(\"\\n\\nMUTUAL INFORMATION FOR BANK-MARKETING ==================================\")\n",
    "plot_mutual_information(bank_data)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.a**\n",
    "\n",
    "The plots above show that both datasets exhibit features which are clearly 'better' for classification that others. Note that Mutual Information only represents the relative potential of a feature as an independent predictor of the class. It does not represent any interactions between features. Furthermore, mutual information by itself does not determine usefulness as it depends on how the model learns the feature. Since we are using the Naive Bayes classifier, and therefore 'learning' the values of all observed features, it is fairly useful.\n",
    "\n",
    "For the Obesity dataset is is clear that features such as 'family_history_with_overweight', 'CAEC' and 'FAVC' are comparatively better metrics than 'Gender, 'TUE', and 'CH20' for predicting whether a person is obese. However, the MI values are all below 0.12, which means that the 'best' feature only provides a 12% indication of the label.\n",
    "\n",
    "For the Bank-Marketing dataset it is clear that features such as 'poutcome' and 'month' are much better metrics for predictions, at least when compared to 'default' and 'housing'. However, the MI values for this dataset are all extremely low (0.03). This means that at best, poutcome alone only gives a 3% better indication of the label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.b** \n",
    "\n",
    "The 'Naive' Bayes classifier assumes that the value of a feature is completely independent of any other feature, given the class. For example, if we are predicting size of an object we assume that the weight of the object and the width of the object is not correlated with each other.\n",
    "\n",
    "This is a necessary assumption as if we are to consider the relationship of all features, we would require an enormous amount of data that contains every possible combination of feature values. A single unseen feature/value would greatly diminish or even break the classifier's performance.\n",
    "\n",
    "This assumption can be problematic as it is not representative of the real world. For example, in the Student dataset, one can assume that 'traveltime' affects 'studytime' and 'freetime', which in turn affects the ability for a student to 'goout' or engage in extracurricular 'activities'.  Another example in the Obesity dataset is that we can assume that someone who smokes, consumes alcohol, and frequently consumes high caloric foods would be more likely to have obesity than if they did not partake.\n",
    "\n",
    "The Naive Bayes classifier assumes none of the above are related and so may not capture the nuanced relationship between certain features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Feature Selection and Ethics [4 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features retained: Medu, Fedu, Mjob, Fjob, reason, traveltime, studytime, failures, schoolsup, famsup, paid, activities, nursery, higher, internet, freetime, goout, absences\n",
      "Features removed: school, sex, address, famsize, Pstatus, guardian, romantic, famrel, Dalc, Walc, health\n",
      "\n",
      "Accuracy:  0.4268104776579353\n",
      "Precision (macro avg):  0.45557121144617224\n",
      "Precision (micro avg):  0.4268104776579353\n",
      "Precision (weighted avg):  0.43403965139762685\n",
      "Recall (macro avg):  0.39830158415869565\n",
      "Recall (micro avg):  0.4268104776579353\n",
      "Recall (weighted avg):  0.4268104776579353\n",
      "F-score (macro avg):  0.4142045331314923\n",
      "F-score (micro avg):  0.4268104776579353\n",
      "F-score (weighted avg):  0.426274675390222\n"
     ]
    }
   ],
   "source": [
    "# Write additional code here, if necessary (you may insert additional code cells)\n",
    "data = preprocess('student.csv')\n",
    "\n",
    "edited_data = data.drop(['school', 'sex', 'address', 'famsize', 'Pstatus', 'guardian', 'romantic', 'famrel', 'Dalc', 'Walc', 'health'], axis=1)\n",
    "edited_student_model = train(edited_data)\n",
    "student_predictions = predict(edited_student_model, student_data)\n",
    "\n",
    "student_truth = list(student_data['label'])\n",
    "\n",
    "print(\"Features retained: Medu, Fedu, Mjob, Fjob, reason, traveltime, studytime, failures, schoolsup, famsup, paid, activities, nursery, higher, internet, freetime, goout, absences\")\n",
    "print(\"Features removed: school, sex, address, famsize, Pstatus, guardian, romantic, famrel, Dalc, Walc, health\\n\")\n",
    "\n",
    "# ACCURACY SCORE\n",
    "student_accuracy =  evaluate(\"accuracy\", student_truth, student_predictions[1], pos_label=None, average=None)\n",
    "print(\"Accuracy: \", student_accuracy)\n",
    "\n",
    "# PRECISION\n",
    "student_precision_macro = evaluate(\"precision\", student_truth, student_predictions[1], pos_label=None, average=\"macro\")\n",
    "student_precision_micro = evaluate(\"precision\", student_truth, student_predictions[1], pos_label=None, average=\"micro\")\n",
    "student_precision_wavg = evaluate(\"precision\", student_truth, student_predictions[1], pos_label=None, average=\"weighted\")\n",
    "print(\"Precision (macro avg): \", student_precision_macro)\n",
    "print(\"Precision (micro avg): \", student_precision_micro)\n",
    "print(\"Precision (weighted avg): \", student_precision_wavg)\n",
    "\n",
    "# RECALL\n",
    "student_recall_macro = evaluate(\"recall\", student_truth, student_predictions[1], pos_label=None, average=\"macro\")\n",
    "student_recall_micro = evaluate(\"recall\", student_truth, student_predictions[1], pos_label=None, average=\"micro\")\n",
    "student_recall_wavg = evaluate(\"recall\", student_truth, student_predictions[1], pos_label=None, average=\"weighted\")\n",
    "print(\"Recall (macro avg): \", student_recall_macro)\n",
    "print(\"Recall (micro avg): \", student_recall_micro)\n",
    "print(\"Recall (weighted avg): \", student_recall_wavg)\n",
    "\n",
    "# F-SCORE\n",
    "student_fscore_macro = evaluate(\"f-score\", student_truth, student_predictions[1], pos_label=None, average=\"macro\")\n",
    "student_fscore_micro = evaluate(\"f-score\", student_truth, student_predictions[1], pos_label=None, average=\"micro\")\n",
    "student_fscore_wavg = evaluate(\"f-score\", student_truth, student_predictions[1], pos_label=None, average=\"weighted\")\n",
    "print(\"F-score (macro avg): \", student_fscore_macro)\n",
    "print(\"F-score (micro avg): \", student_fscore_micro)\n",
    "print(\"F-score (weighted avg): \", student_fscore_wavg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.a**\n",
    "\n",
    "Making a decision to admit/not admit a student based solely on the predicted grade is extremely unfair as it will only provide previously high scoring students an opportunity for further studies.\n",
    "\n",
    "Looking only at an estimated grade does not take into account a student's individual circumstances, and does not necessarily predict how a student will perform as a lot of the nuance represented in the feature set (which may explain a student's current grade) will not be properly considered.\n",
    "\n",
    "For example, many of the features in this set relate to things a student cannot change (e.g. address, famsize, traveltime, nursery, internet). Furthermore, many of the features should be irrelevant when making admission decisions (e.g. sex, Medu/Fedu, Mjob/Fjob, romantic, Dalc/Walc). The features also represent ingrained cultural expectations which affect a student's capacity for education (e.g. famsize may allow for certain members to pursue education while others support the household)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.b**\n",
    "\n",
    "The performance of the NB classifier using the reduced-feature set does not perform as well as the NB classifier using the full-feature set. The reduced-feature version predicts the correct class label 42.7% of the time, while the full-featured version predicts the correct class 48.4% of the time. \n",
    "\n",
    "This discrepancy shows that my judgement on what is considered ethical/not ethical removed some valuable information which may have assisted in the classification task. In particular, I believe the most important (and unethical features) which I removed include the student's sex, address, Medu/Fedu, Mjob/Fjob and health. Although the NB classifier asserts that each feature is independent from the other, the feature set above is almost definitely related in the real world and would have a large influence on other features of this data set.\n",
    "\n",
    "Overall, my judgement did not result in a feature set which could better explain a students grades. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.c**\n",
    "\n",
    "Fairness through unawaress does not guarantee a fair classifier. It only removes certain attributes that may be directly discriminatory, but background knowledge or awareness of certain features can likely still be assumed based on the values of other related features.\n",
    "\n",
    "For example, if I remove all information like school, sex, Pstatus, Medu, Fedu, Mjob, and Fjob from the student dataset, but keep address (for arguments sake assume it is recorded as a postcode); we can likely still infer that a student comes from a priveleged background if they live in Toorak. \n",
    "\n",
    "Fairness through unawareness also strengthens the independence assumption, which clearly does not hold in real-life scenarios. Furthermore, in certain scenarios, removing ethically problematic features from a dataset may be counter productive to achieve an ethical goal (e.g. university admission for underrepresented students). A careful consideration of each feature set is required to find a 'fair' classifier for each task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Authorship Declaration</b>:\n",
    "\n",
    "   (1) I certify that the program contained in this submission is completely\n",
    "   my own individual work, except where explicitly noted by comments that\n",
    "   provide details otherwise.  I understand that work that has been developed\n",
    "   by another student, or by me in collaboration with other students,\n",
    "   or by non-students as a result of request, solicitation, or payment,\n",
    "   may not be submitted for assessment in this subject.  I understand that\n",
    "   submitting for assessment work developed by or in collaboration with\n",
    "   other students or non-students constitutes Academic Misconduct, and\n",
    "   may be penalized by mark deductions, or by other penalties determined\n",
    "   via the University of Melbourne Academic Honesty Policy, as described\n",
    "   at https://academicintegrity.unimelb.edu.au.\n",
    "\n",
    "   (2) I also certify that I have not provided a copy of this work in either\n",
    "   softcopy or hardcopy or any other form to any other student, and nor will\n",
    "   I do so until after the marks are released. I understand that providing\n",
    "   my work to other students, regardless of my intention or any undertakings\n",
    "   made to me by that other student, is also Academic Misconduct.\n",
    "\n",
    "   (3) I further understand that providing a copy of the assignment\n",
    "   specification to any form of code authoring or assignment tutoring\n",
    "   service, or drawing the attention of others to such services and code\n",
    "   that may have been made available via such a service, may be regarded\n",
    "   as Student General Misconduct (interfering with the teaching activities\n",
    "   of the University and/or inciting others to commit Academic Misconduct).\n",
    "   I understand that an allegation of Student General Misconduct may arise\n",
    "   regardless of whether or not I personally make use of such solutions\n",
    "   or sought benefit from such actions.\n",
    "\n",
    "   <b>Signed by</b>: Shervyn Singh (1236509)\n",
    "   \n",
    "   <b>Dated</b>: 02/09/2022"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
